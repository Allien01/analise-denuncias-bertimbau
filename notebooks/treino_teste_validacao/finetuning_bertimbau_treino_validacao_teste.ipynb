{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Kkt3yw00IoE"
   },
   "source": [
    "@inproceedings{souza2020bertimbau,\n",
    "  author    = {F{\\'a}bio Souza and\n",
    "               Rodrigo Nogueira and\n",
    "               Roberto Lotufo},\n",
    "  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},\n",
    "  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},\n",
    "  year      = {2020}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AYpHg_BMl-pZ"
   },
   "source": [
    "# **Conectando ao Google Drive**\n",
    "\n",
    "\n",
    "Para acessar e carregar os dados necess√°rios para o projeto, montamos o Google Drive. Isso permite que o notebook leia arquivos diretamente de uma pasta em sua conta, facilitando o gerenciamento e a persist√™ncia dos dados entre diferentes sess√µes. O comando a seguir solicita permiss√£o para conectar o ambiente do Colab ao seu Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey0-HM5ocnvu",
    "outputId": "6c2f29f6-bbbb-40c8-c3d8-36f0cb7e4657"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcTbLWwTr74s",
    "outputId": "ba691631-ddb6-4a61-c2e5-20066f609ecb"
   },
   "outputs": [],
   "source": [
    "!pip install transformers evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feHRGfqdONsa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "start_total_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkRUcLU1PHke"
   },
   "source": [
    "# **1. Prepara√ß√£o dos Dados: Dividindo e balanceando o dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSTIVroTlVJi"
   },
   "source": [
    "Nesta etapa, o conjunto de dados √© preparado para o treinamento do modelo. O ***arquivo denuncias_balanceadas.xlsx*** √© dividido em tr√™s partes: treino (70%), valida√ß√£o (15%) e teste (15%), garantindo que cada conjunto tenha uma distribui√ß√£o similar das classes ***invasao_domicilio*** e ***violencia_fisica***.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3O3x60E3PHtT",
    "outputId": "52166314-7ce4-4a7d-9fc9-d5e8fa60abef"
   },
   "outputs": [],
   "source": [
    "# Carregar dataset j√° processado\n",
    "file_path = '/content/drive/MyDrive/2025/tcc-final/denuncias_balanceadas.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Dividir em treino (70%), valida√ß√£o (15%) e teste (15%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['classe'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['classe'], random_state=42)\n",
    "\n",
    "print(f\"Treino: {len(train_df)}, Valida√ß√£o: {len(val_df)}, Teste: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE3WAEgOPYHa"
   },
   "source": [
    "# **2. Tokeniza√ß√£o com BERTimbau**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLF_TF9omQw7"
   },
   "source": [
    "Para que o modelo BERTimbau entenda nossos textos, √© preciso convert√™-los em um formato num√©rico. Este processo, chamado de tokeniza√ß√£o, utiliza o ***AutoTokenizer*** do modelo BERTimbau, que √© um modelo de linguagem pr√©-treinado especificamente para o portugu√™s do Brasil. O ***tokenizer*** quebra os textos em ***tokens*** (palavras ou partes de palavras) e os mapeia para n√∫meros que o modelo pode processar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324,
     "referenced_widgets": [
      "f4c01dbd10ff4a63b3568b88bca837d0",
      "82e982f8ae6b438fb15ed2824767ff9d",
      "9609288eac6b4766a03a4876280804c2",
      "b0e6b48b8bd249aaad06e261a70124f9",
      "ac6f7a1f76ed4302ba554d4db6da41db",
      "e581077d73b64282b71cc239e251f58e",
      "2a56836cc5754c75b47ced336a02ae4c",
      "ac390c7ca1054c6b9f905c595a6493b9",
      "5af833a246d541ec9a85aab48ab8c95a",
      "6a53959fea0d4203b9ad9256398d649f",
      "28809642f5d7431a8f596049db8bf122",
      "b0689823bdfc49049454abb5e15c694e",
      "a5c44f467800441fa0dee5b40030ddf9",
      "3d48b04e48cf417b807e4af641ec2a79",
      "09618881b6564ec5b316e5d032b80df7",
      "b4d8b0af1d11422e88ea4d7070059d9d",
      "7cbaccdb8fed428aaa0a292fbadbb250",
      "b4ded4c8ba0342b6a17727f119c66653",
      "7b26081c938849829c8d53024de432db",
      "5092dcd2708a489ea0abd3139deb12e6",
      "dfb1588a08924fdfaacf998a84c583d5",
      "407fb74cb8994b9288d97ee705cc11b5",
      "dd4fe6b985aa4ea28c34d87b78139bfd",
      "b92536c3bad64b3e927da79277a2cadd",
      "2d80327183b9453ea3629279787a3dfc",
      "d54de5307f754d1099c329a8fa4ec977",
      "556fd3e84a4946c08419c4512b12f927",
      "abc17455f556444c8798903bcda165cc",
      "4230481840144e79970e78eb5b83fcba",
      "47c45fe300d147d8b43315b7889eb380",
      "d199685bd22c447db408b9263bfd14d2",
      "3e097432f0ce48a48160a026a231c1e3",
      "3814c643a6ca41f6ac6b4b4dd2527468",
      "d35b0c6163b0433fa99d39837e01fb0a",
      "53f8f70bf1884c2096adb9456d67a2fc",
      "979df651341740949417218488cc80e8",
      "bdee3adc247d4feb87730ed908ce52f4",
      "2f002fc67d2348c0a77385ca4522c02e",
      "b2275ec603f94f748906222a2fd5edfa",
      "61a92b048db8404791bf38eb71a7d463",
      "497a619981bc49f6adca823fc2749108",
      "5fae2a6d353b47e781b6920da79cb661",
      "cb61118eaa9447b79915e81dae12222f",
      "e58ce6742743420a9e282ea9bcf3b8e3",
      "6ce67fcc08914334a0201dd4c41cd98f",
      "fe4d8bc1bb62426da28aa5171e65fa49",
      "adff9e6d58384b86a00fbed5ff0d19d3",
      "af6fe33c83ca49629c6962b8efa9a1c0",
      "4b911fdfe39d49ee95c49e24ff1d1e54",
      "03803f5a73564f828c6f2b3405a7c3a7",
      "84fb4dad99984caea2a426ea8100b6b3",
      "7500c834b7b7414a81639573581a2d8d",
      "30e4e04bc8954e65ab43e4e03ea89bd7",
      "6c596bcb5179465492dead9858aa66b7",
      "f48a14296a1646f5b0eda5d3f79dcbd3"
     ]
    },
    "id": "sql7RSXMPd1g",
    "outputId": "e624dcbc-ccb7-475e-a68d-8cc5b067de94"
   },
   "outputs": [],
   "source": [
    "model_path = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Fun√ß√£o de tokeniza√ß√£o\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"texto\"], truncation=True,max_length=512, padding='longest',return_tensors='pt')\n",
    "\n",
    "# Tokenizar os DataFrames diretamente (sem usar datasets.Dataset)\n",
    "train_encodings = tokenize_function(train_df.to_dict('list'))\n",
    "val_encodings = tokenize_function(val_df.to_dict('list'))\n",
    "test_encodings = tokenize_function(test_df.to_dict('list'))\n",
    "print(train_encodings[0])\n",
    "print(val_encodings[0])\n",
    "print(test_encodings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0mzN3JPP-s4"
   },
   "source": [
    "# **3. Configura√ß√£o do Modelo: Adaptando o BERTimbau para a nossa tarefa**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hXAZ46UOm9oC"
   },
   "source": [
    "Aqui, carregamos o modelo BERTimbau pr√©-treinado e o adaptamos para a nossa tarefa de classifica√ß√£o de texto. A parte mais importante desta etapa √© a configura√ß√£o da camada final (chamada de ***classifier***), que ser√° treinada para prever uma de nossas duas classes. Al√©m disso, congelamos as camadas de base do BERT, o que significa que apenas a nova camada do classificador ser√° ajustada, otimizando o treinamento para um conjunto de dados menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114,
     "referenced_widgets": [
      "4fa82c3ec1d24ba19e9f44f5e5850d61",
      "f01dba91575c4849a2dd9a34c5124a3d",
      "12c4fe4736e240b7af40219363e51119",
      "8c51ef584d5548998bf4012f808bdba1",
      "1741adfcbb2e4351ab659531cb84791b",
      "a24fe14295424fb995bac69c81e6c285",
      "533d5cfeba034752af884a401da924dc",
      "2b32465c74d644a18bee70b45951c661",
      "c526bdfd9906473fa0395e10320f6e5f",
      "3205a83ff66b4e51868e0866db972695",
      "d4f6caedf9fd4314b6263f4e1e1a32a7",
      "0f26300fec2743738f0f444952162fcc",
      "ad56db916b33439593b141f9b5f37cb2",
      "1ed4987eedaa4dc1b5004ed575f90dda",
      "8f1545e5cfa34d1aaab0959ef06e59a2",
      "e1bf931d3f414dd0aafd2d80f30a1c20",
      "83b3fe6c71c84a3baac7579f0598449a",
      "f33734c6da9940eb9ceb790ae8447cd5",
      "00fa381cad774da4905e9e76a889cc56",
      "03c9833277694a17aae716b310978204",
      "fb8c95decb01436d806dad47a198520e",
      "fb021724c85146e7bf94175f55870b56"
     ]
    },
    "id": "tkQIL_4sYTMq",
    "outputId": "dd721531-3843-45b1-e648-be20c05454eb"
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"invasao_domicilio\", 1: \"violencia_fisica\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Congelar camadas do BERT (exceto pooler e classificador)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name and 'pooler' not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S-nz2HQZ0CA"
   },
   "source": [
    "# **4. Avalia√ß√£o do Modelo: Definindo as m√©tricas de sucesso**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26ujEjghnObt"
   },
   "source": [
    "Para saber se o nosso modelo est√° aprendendo bem, precisamos de m√©tricas de avalia√ß√£o. Al√©m da ***acur√°cia*** (a porcentagem de acertos), calculamos outras m√©tricas importantes, como a ***AUC-ROC***, que mede a capacidade do modelo de distinguir entre as classes, e a ***Precis√£o, Recall e F1-Score***, que nos d√£o uma vis√£o mais detalhada sobre o desempenho do modelo em cada classe (invas√£o de domic√≠lio e viol√™ncia f√≠sica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176,
     "referenced_widgets": [
      "611f8591b3dc468eaf7e3b9db92b2158",
      "f83f49ac3b804e32b299edcc22335438",
      "a6e2359e8eec45a784d497352d8fdbc0",
      "3f7545b1de72444cb280d2c93425bc10",
      "fcb53d4911b448479fb83cbea8a75f2f",
      "4e3ea049c4a9499ca6ba3e3f3170569d",
      "afc02c7a69fa4daab63897646f8a8296",
      "d57b88483ce0422c9ac7878af699cc30",
      "139f47b36ff94018be45c19ae4df9134",
      "802edbb7a6f245dca8a951046c36ff5d",
      "f1c67662d682429084cdc3f28f958282",
      "0acb0219143341798e690ad87f4ab8de",
      "13462cf93eed4ee0b8c8fa6ba031db59",
      "01f61aecd0d14b4c98a6b5a727a81b4e",
      "a04a633beb9649ce9f5cbb76f8ccdab5",
      "0b0f33fb0cdc48d68b9d05ae120155e6",
      "e9fee64d4c8f4f789b5e30732244eb17",
      "55c570e48cc846c3a822ae48c22cfe90",
      "10b0f68e1f0545108f2f2822fd33d167",
      "27b34391260c49799f29c8d59489b3ba",
      "c5f78cff6ece4c0c8e78d8375b52e641",
      "3a1a5dba55a34ac0ab384736e7aa3006",
      "9d360e3cc0864ac1bcc1e1d6524f996d",
      "84fc02a486364be79708c60b90e0f8bc",
      "af210157d1a04fa9b631db39e6e8a446",
      "15edacd88b004428a07e9dd24924bbe1",
      "edddd8053b8148d483285c883d569ff1",
      "da1f5abbc8a7433b88b62215768d2057",
      "651c548201aa472ebc71c9b9b26e0f38",
      "55a43b33952d4a98bf7cd45f9084865d",
      "72ad07b94b0e4ff1adbf3c31d069cad4",
      "9dc8155cb1984f5896f35a55b8adbd4e",
      "4ea6409d86534e5780dab0c35eb9811e",
      "293bd8c7d3ba4c7ba30cfb3170231c98",
      "13f9a78e712c40e3bf69de88143c6316",
      "8c097111a0934447a3f0f14bfda4ab12",
      "4af8302c9c654cb7a1309c12aeb0eb49",
      "eee2b0685cd947e197a858da8347b478",
      "63e79c2f26b0494fb6a7bcb7a5018bc7",
      "3f8cff1a42cd4c9ba00ae164e0f27245",
      "a6f66d5be32e4a5fb09dfd210b7e61a6",
      "0c2802862e1e4938b816d0445e3d98dd",
      "bdb80ae962ef43d49a5c84b0402a5d96",
      "082c4efeed9144e49601fcc55acbd119",
      "9503e5feaef6474bbb3d6d428d7ca52c",
      "9366c7b11c3f41d8aec3a0c8252c4b09",
      "c759002eab0c4fb3acedaffc6429995c",
      "74b548a4959d4bd1aa64ce8af52c89c1",
      "ea0dc6886e1b47b59ca7ff89f39f8ed1",
      "c8e539933a924edd8c5e2d548ab99fdc",
      "5c06b4ca1a814d8eb612785995c7f5bf",
      "65b4c66f31184d83a4b4feb25260e65e",
      "95fbe6dab32747dbbbeb159625783d1a",
      "e5d695355ff84dbfaa4847cf75bbc853",
      "a91d4b4bb4304692ac5a30961601e5d5"
     ]
    },
    "id": "F-gctrIbZ1Pv",
    "outputId": "a7c1c91e-382d-4f31-bf58-274e6e205e62"
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "auc_score = evaluate.load(\"roc_auc\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        probabilities = np.exp(predictions - np.max(predictions, axis=-1, keepdims=True)) / np.sum(np.exp(predictions - np.max(predictions, axis=-1, keepdims=True)), axis=-1, keepdims=True)\n",
    "        positive_class_probs = probabilities[:, 1]\n",
    "\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "        acc = accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "        auc = auc_score.compute(prediction_scores=positive_class_probs, references=labels)[\"roc_auc\"]\n",
    "\n",
    "        f1_results = f1_metric.compute(\n",
    "            predictions=preds,\n",
    "            references=labels,\n",
    "            average=None,\n",
    "            labels=[label2id[\"invasao_domicilio\"], label2id[\"violencia_fisica\"]]\n",
    "        )\n",
    "        precision_results = precision.compute(\n",
    "            predictions=preds,\n",
    "            references=labels,\n",
    "            average=None,\n",
    "            labels=[label2id[\"invasao_domicilio\"], label2id[\"violencia_fisica\"]]\n",
    "        )\n",
    "        recall_results = recall.compute(\n",
    "            predictions=preds,\n",
    "            references=labels,\n",
    "            average=None,\n",
    "            labels=[label2id[\"invasao_domicilio\"], label2id[\"violencia_fisica\"]]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": round(acc, 4),\n",
    "            \"auc\": round(auc, 4),\n",
    "            \"f1_invasao\": round(f1_results[\"f1\"][0], 4),\n",
    "            \"f1_violencia\": round(f1_results[\"f1\"][1], 4),\n",
    "            \"precision_invasao\": round(precision_results[\"precision\"][0], 4),\n",
    "            \"precision_violencia\": round(precision_results[\"precision\"][1], 4),\n",
    "            \"recall_invasao\": round(recall_results[\"recall\"][0], 4),\n",
    "            \"recall_violencia\": round(recall_results[\"recall\"][1], 4),\n",
    "            \"precision_macro\": round(np.mean(precision_results[\"precision\"]), 4),\n",
    "            \"recall_macro\": round(np.mean(recall_results[\"recall\"]), 4),\n",
    "            \"f1_macro\": round(np.mean(f1_results[\"f1\"]), 4)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no c√°lculo de m√©tricas: {str(e)}\")\n",
    "        return {\"accuracy\": 0.0, \"auc\": 0.0, \"f1_invasao\": 0.0, \"f1_violencia\": 0.0,\n",
    "                \"precision_invasao\": 0.0, \"precision_violencia\": 0.0,\n",
    "                \"recall_invasao\": 0.0, \"recall_violencia\": 0.0,\n",
    "                \"precision_macro\": 0.0, \"recall_macro\": 0.0, \"f1_macro\": 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fmwNJA-Z5DK"
   },
   "source": [
    "# **5. Treinamento: A m√°gica do Fine-Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "005QPEjPntGt"
   },
   "source": [
    "Nesta se√ß√£o, iniciamos o treinamento do modelo usando a biblioteca ***Hugging Face Trainer***. O treinamento ser√° executado por 30 √©pocas. O ***EarlyStoppingCallback*** √© ativado para interromper o treinamento caso o desempenho do modelo no conjunto de valida√ß√£o pare de melhorar por tr√™s √©pocas consecutivas, evitando o overfitting. Voc√™ pode acompanhar a evolu√ß√£o da perda (loss) e da acur√°cia durante o processo no gr√°fico abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Lu5KpVfWaGA3",
    "outputId": "688c46c6-0023-4096-f4c9-931a97b070f1"
   },
   "outputs": [],
   "source": [
    "train_losses_plot, val_losses_plot, test_losses_plot = [], [], []\n",
    "train_accuracies_plot, val_accuracies_plot, test_accuracies_plot = [], [], []\n",
    "\n",
    "# Configura√ß√£o dos argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bertimbau-denuncias\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=30,\n",
    "    eval_strategy=\"epoch\",        # Avaliar no conjunto de valida√ß√£o ao final de cada √©poca\n",
    "    save_strategy=\"epoch\",        # Salvar checkpoint ao final de cada √©poca\n",
    "    logging_strategy=\"epoch\", # Registrar m√©tricas ao final de cada √©poca\n",
    "    load_best_model_at_end=True,  # Carregar o melhor modelo (baseado na m√©trica de valida√ß√£o) ao final do treino\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",             # Desativar report para plataformas como Weights & Biases\n",
    "    fp16=True,                    # Habilitar mixed precision training\n",
    "    seed=42,\n",
    "    gradient_accumulation_steps=1,\n",
    "    remove_unused_columns=True,\n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "# Data Collator para padding din√¢mico\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Custom Dataset (para trabalhar com DataFrames)\n",
    "class CustomDataset(TorchDataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        # Converte para tensor e garante que encodings.items() tem os valores corretos\n",
    "        self.encodings = {key: torch.tensor(val) for key, val in encodings.items()}\n",
    "        # Converte labels para tensor, lidando com pd.Series ou listas/arrays\n",
    "        self.labels = torch.tensor(labels.values if isinstance(labels, pd.Series) else labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Criar datasets\n",
    "# Assegure que train_df, val_df, test_df e label2id estejam definidos.\n",
    "# .tolist() √© importante para garantir que labels sejam um tipo Python nativo antes de converter para tensor no CustomDataset.\n",
    "train_dataset = CustomDataset(train_encodings, train_df['classe'].map(label2id).tolist())\n",
    "val_dataset = CustomDataset(val_encodings, val_df['classe'].map(label2id).tolist())\n",
    "test_dataset = CustomDataset(test_encodings, test_df['classe'].map(label2id).tolist())\n",
    "\n",
    "# Instanciar o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset, # O trainer usar√° este para avalia√ß√£o autom√°tica por √©poca\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold = 1e-3)],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"\\nüöÄ Iniciando treinamento...\")\n",
    "\n",
    "    # O trainer.train() ir√° executar o treinamento e as avalia√ß√µes de valida√ß√£o\n",
    "    # por √©poca, al√©m de carregar o melhor modelo no final.\n",
    "    train_results = trainer.train()\n",
    "\n",
    "    # --- Extrair m√©tricas dos logs para os gr√°ficos ---\n",
    "    # Os logs cont√™m as m√©tricas de treino e valida√ß√£o por √©poca\n",
    "    logs = trainer.state.log_history\n",
    "\n",
    "    # Iterar sobre os logs para preencher as listas de plotagem\n",
    "    for log_entry in logs:\n",
    "        # Perdas de Treino: o 'loss' √© logado para o treinamento\n",
    "        if 'loss' in log_entry and 'eval_loss' not in log_entry:\n",
    "            train_losses_plot.append(log_entry['loss'])\n",
    "\n",
    "        # M√©tricas de Valida√ß√£o: logadas com prefixo 'eval_'\n",
    "        if 'eval_loss' in log_entry and 'eval_accuracy' in log_entry:\n",
    "            val_losses_plot.append(log_entry['eval_loss'])\n",
    "            val_accuracies_plot.append(log_entry['eval_accuracy'])\n",
    "\n",
    "    # --- Avalia√ß√£o final do melhor modelo no conjunto de teste ---\n",
    "    # Este √© o ponto mais importante para as m√©tricas finais do seu modelo.\n",
    "    print(\"\\nüß™ Avalia√ß√£o final do melhor modelo no conjunto de teste...\")\n",
    "    final_test_metrics = trainer.evaluate(test_dataset)\n",
    "\n",
    "    # --- üìä Gr√°ficos de compara√ß√£o por √©poca ---\n",
    "\n",
    "    # O n√∫mero de √©pocas para plotagem ser√° o n√∫mero de avalia√ß√µes de valida√ß√£o registradas\n",
    "    epochs_completed = len(val_accuracies_plot)\n",
    "    epochs_to_plot = list(range(1, epochs_completed + 1))\n",
    "\n",
    "    train_losses_for_plot = train_losses_plot[-epochs_completed:] if len(train_losses_plot) >= epochs_completed else train_losses_plot\n",
    "\n",
    "    if not epochs_to_plot:\n",
    "        print(\"\\nN√£o h√° dados suficientes para gerar os gr√°ficos. Verifique o processo de treinamento e coleta de m√©tricas.\")\n",
    "    else:\n",
    "        # Loss por √©poca (Treino, Valida√ß√£o)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(epochs_to_plot, train_losses_for_plot, label=\"Treino\", marker='o')\n",
    "        plt.plot(epochs_to_plot, val_losses_plot, label=\"Valida√ß√£o\", marker='o')\n",
    "        # Plotar o resultado final do teste como um ponto ou linha horizontal.\n",
    "        plt.xlabel(\"√âpoca\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss por √âpoca (Treino, Valida√ß√£o)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training or evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFSsNAWvaQQs"
   },
   "source": [
    "# **6. AVALIA√á√ÉO FINAL NO TESTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "0a5gLzc6aQuU",
    "outputId": "bcdcb4ff-ec78-444a-82d5-e42548d4f3a7"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n==================== RESULTADOS FINAIS DO MODELO ====================\")\n",
    "print(\"\\nAvalia√ß√£o no conjunto de teste:\")\n",
    "\n",
    "test_results = trainer.predict(test_dataset)\n",
    "final_metrics = test_results.metrics\n",
    "\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SLL_XiZaSle"
   },
   "source": [
    "# **7. SALVAR MODELO E DADOS DE RESULTADOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbEnj8O5aUmx",
    "outputId": "66a52d9a-d7d8-4d35-ea35-9d742b4b561c"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/content/drive/MyDrive/2025/tcc-final/resultados/modelo_bertimbau_final\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/2025/tcc-final/resultados/modelo_bertimbau_final\")\n",
    "print(\"Modelo salvo no Google Drive!\")\n",
    "\n",
    "# Salvar resultados do treinamento e r√≥tulos/previs√µes finais\n",
    "output_results_path = \"/content/drive/MyDrive/2025/tcc-final/resultados/resultados_fine_tuning.xlsx\"\n",
    "\n",
    "# Get the predictions from the test_results object\n",
    "predictions = np.argmax(test_results.predictions, axis=1)\n",
    "\n",
    "\n",
    "# Criar um DataFrame para os resultados de teste\n",
    "results_df = pd.DataFrame({\n",
    "    'texto': test_df['texto'],\n",
    "    'rotulo_verdadeiro': test_df['classe'],\n",
    "    'rotulo_predito_id': predictions,\n",
    "    'rotulo_predito': [id2label[p] for p in predictions]\n",
    "})\n",
    "\n",
    "# Salvar o DataFrame de resultados em um arquivo Excel\n",
    "results_df.to_excel(output_results_path, index=False)\n",
    "print(f\"Resultados de teste (texto, r√≥tulo verdadeiro, r√≥tulo predito) salvos em: {output_results_path}\")\n",
    "\n",
    "# Salvar as m√©tricas finais em um arquivo de texto ou CSV\n",
    "metrics_output_path = \"/content/drive/MyDrive/2025/tcc-final/resultados/metricas_finais.txt\"\n",
    "with open(metrics_output_path, 'w') as f:\n",
    "    f.write(\"M√©tricas Finais da Avalia√ß√£o no Conjunto de Teste:\\n\")\n",
    "    for metric_name, value in final_metrics.items():\n",
    "        f.write(f\"{metric_name}: {value:.4f}\\n\")\n",
    "print(f\"M√©tricas finais salvas em: {metrics_output_path}\")\n",
    "\n",
    "# --- FIM DO CONTADOR DE TEMPO ---\n",
    "end_total_time = time.time()\n",
    "total_execution_time = end_total_time - start_total_time\n",
    "print(f\"\\nTempo total de execu√ß√£o do script: {total_execution_time:.2f} segundos\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
