{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Kkt3yw00IoE"
   },
   "source": [
    "@inproceedings{souza2020bertimbau,\n",
    "  author    = {F{\\'a}bio Souza and\n",
    "               Rodrigo Nogueira and\n",
    "               Roberto Lotufo},\n",
    "  title     = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese},\n",
    "  booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)},\n",
    "  year      = {2020}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6yWjyiwu4vI"
   },
   "source": [
    "# **Conectando ao Google Drive**\n",
    "\n",
    "Para acessar e carregar os dados necess√°rios para o projeto, montamos o Google Drive. Isso permite que o notebook leia arquivos diretamente de uma pasta em sua conta, facilitando o gerenciamento e a persist√™ncia dos dados entre diferentes sess√µes. O comando a seguir solicita permiss√£o para conectar o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey0-HM5ocnvu",
    "outputId": "952c1815-f6dc-4bea-d4fe-583923280c67"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XcTbLWwTr74s",
    "outputId": "e7810a61-18d7-4123-de03-e2f4afa775f9"
   },
   "outputs": [],
   "source": [
    "!pip install transformers evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "feHRGfqdONsa"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "start_total_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkRUcLU1PHke"
   },
   "source": [
    "# **1. Prepara√ß√£o dos Dados: Dividindo e balanceando o dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vUMiCRBvDni"
   },
   "source": [
    "Nesta etapa, o conjunto de dados √© preparado para o treinamento do modelo. O ***arquivo denuncias_balanceadas.xlsx*** √© dividido em tr√™s partes: treino (70%), valida√ß√£o (15%) e teste (15%), garantindo que cada conjunto tenha uma distribui√ß√£o similar das classes ***invasao_domicilio*** e ***violencia_fisica***.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3O3x60E3PHtT",
    "outputId": "c35bc14a-8a09-4340-f38f-7c552cdc3f78"
   },
   "outputs": [],
   "source": [
    "# Carregar dataset j√° processado\n",
    "file_path = '/content/drive/MyDrive/2025/tcc-final/denuncias_balanceadas.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Dividir em treino (70%), valida√ß√£o (15%) e teste (15%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, stratify=df['classe'], random_state=42)\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['classe'], random_state=42)\n",
    "\n",
    "print(f\"Treino: {len(train_df)}, Valida√ß√£o: {len(val_df)}, Teste: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE3WAEgOPYHa"
   },
   "source": [
    "# **2. Tokeniza√ß√£o com BERTimbau**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nN3N0mpWvRBf"
   },
   "source": [
    "Para que o modelo BERTimbau entenda nossos textos, √© preciso convert√™-los em um formato num√©rico. Este processo, chamado de tokeniza√ß√£o, utiliza o ***AutoTokenizer*** do modelo BERTimbau, que √© um modelo de linguagem pr√©-treinado especificamente para o portugu√™s do Brasil. O ***tokenizer*** quebra os textos em ***tokens*** (palavras ou partes de palavras) e os mapeia para n√∫meros que o modelo pode processar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 324,
     "referenced_widgets": [
      "e8a8f98e4e974a28a14c122a3aaf0044",
      "e318073ca2014ee39f3553d5ef47bb67",
      "922e242ae2d44e34bfbf60b27dcce996",
      "f334b04e737744abae6ea1289bf64d71",
      "235b604c66bc4fdab5dde9dc21a42bac",
      "c5f27c8e10f64c749c8fa0c02967f43d",
      "7121302456eb40eaac682ed6cb27da27",
      "78d424b3cee94cfbb9d8bcdbf3cca820",
      "f994513fa72d41418175853ef2eda484",
      "4d9d84c53c9840e9bf781155341e4a63",
      "5bde11f86b5449d9a5c58a1971115453",
      "aadca8b7ee9d4dcda3f18d57590315d3",
      "bda82f7c71d94bd995b961f7bb8e5228",
      "210e6ab086de4d659e5b1df0c8af9bfc",
      "65ba870da4a242e9939714078900bbff",
      "79c293a1faf148de9d02a7af323859fd",
      "dda6dd99156047309771f83bd586516f",
      "3f6e2cbb56f54155beba93cc2a54d086",
      "6297d57dcdde409bbd047489e1923a4c",
      "1b6a2c8c98d842fca2804c7a2f885fe1",
      "2e515666a060417db9847173a931c887",
      "13374ea3e37a48f78a295eea9336bc46",
      "72c63eb8ad0d4d1b8cd89c7e3f96c5f3",
      "a19e913d257540a891cc1d4ef683ddf0",
      "b489b166c5494313bce14ff6a77ea2e5",
      "54dcaf5227bd4be5bec779ee020be6bb",
      "daae856ceb564fc18cc0e78c183868ce",
      "2c79010e368f4a44988d84188d2ac3c4",
      "55a8237d30c748a48c2ddcd150359d98",
      "4ba033bfa69c45068ab88f0033dbecbc",
      "ba4377bb83514eecb7bfec718f059da9",
      "7ae17722fb004c98a1a5e580c716d7b1",
      "7ffd5b4abac44afcae383255504b6545",
      "e55952e49cf64865ba16e650c9cca127",
      "70b547e8a6d043fe97e0fcd968cc33eb",
      "0c9e59ac393b45d18ba30c1833394fb8",
      "1032ab81786b4f2e898ff77e4489b482",
      "e90fa4dbbf4145609c693d94499a827d",
      "e88782560503455c8a24177232176d8e",
      "207097c867ff4f958ce228662ae6e711",
      "bd72b44de89c4c4783579d7c6f478f6d",
      "8eaf555c6c2043f7975715692327075e",
      "7908ee6812c94b9786591259990ed9f0",
      "ad079cca965045038dfd468293ac8342",
      "da906eb9519841df8691c7bb65551ea6",
      "138852093868434ab3da34b3887c45d9",
      "98ba881269864bf0abcc0fc54b6aa13c",
      "f789a47a1eb4495cb9a16f656259bfc7",
      "6b139acbbc6a402c9165390ca7767336",
      "8012196ce68644fbb2b0661c938fe07f",
      "75747e784ccd4bcf9f749c280841a605",
      "a40486d793684d79aba8b1cd29c69680",
      "5434d796908e4f5d9cc2c6491c4bcde5",
      "ab5ed2d9cb244a038dfe130c0d397176",
      "4bbebb05584244ad8aadb70b29cc3ca4"
     ]
    },
    "id": "sql7RSXMPd1g",
    "outputId": "ae8defb7-c3ac-47c8-b57b-11d4e31586a2"
   },
   "outputs": [],
   "source": [
    "model_path = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Fun√ß√£o de tokeniza√ß√£o\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"texto\"], truncation=True,max_length=512, padding='longest',return_tensors='pt')\n",
    "\n",
    "# Tokenizar os DataFrames diretamente (sem usar datasets.Dataset)\n",
    "train_encodings = tokenize_function(train_df.to_dict('list'))\n",
    "val_encodings = tokenize_function(val_df.to_dict('list'))\n",
    "test_encodings = tokenize_function(test_df.to_dict('list'))\n",
    "print(train_encodings[0])\n",
    "print(val_encodings[0])\n",
    "print(test_encodings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3KiSVUFvJ3O"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0mzN3JPP-s4"
   },
   "source": [
    "# **3. Configura√ß√£o do Modelo: Adaptando o BERTimbau para a nossa tarefa**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLz0CWZdvZAW"
   },
   "source": [
    "Aqui, carregamos o modelo BERTimbau pr√©-treinado e o adaptamos para a nossa tarefa de classifica√ß√£o de texto. A parte mais importante desta etapa √© a configura√ß√£o da camada final (chamada de ***classifier***), que ser√° treinada para prever uma de nossas duas classes. Al√©m disso, congelamos as camadas de base do BERT, o que significa que apenas a nova camada do classificador ser√° ajustada, otimizando o treinamento para um conjunto de dados menor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 82,
     "referenced_widgets": [
      "fcebd7e061904100b45940683ee207cb",
      "c00967d628e34217a184abe03bf44a8d",
      "bf4e9a56a6c64a2c9c878598393f6911",
      "692e91f33c7949dbbe152c0e6b177e6c",
      "e31572308a894b12a15923f1abe9b85a",
      "c3f09544647745b482332f46ae1d560c",
      "06660b1fc06441e894a041a4f43728a4",
      "7f06ea39a65147d59045d59c11b4a8fd",
      "a7dbd9284c75483ba4df40fe2642886c",
      "b38db9c0f9a14666a3384898085de602",
      "beaedaa2482243388bf50738f995e78d"
     ]
    },
    "id": "tkQIL_4sYTMq",
    "outputId": "dd05d6b2-9b5f-4599-f56f-8711991c608c"
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"invasao_domicilio\", 1: \"violencia_fisica\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_path,\n",
    "    num_labels=2,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# Congelar camadas do BERT (exceto pooler e classificador)\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' not in name and 'pooler' not in name:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S-nz2HQZ0CA"
   },
   "source": [
    "#**4. Avalia√ß√£o do Modelo: Definindo as m√©tricas de sucesso**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vU5sR5zSvwlz"
   },
   "source": [
    "Para saber se o nosso modelo est√° aprendendo bem, precisamos de m√©tricas de avalia√ß√£o. Al√©m da ***acur√°cia*** (a porcentagem de acertos), calculamos outras m√©tricas importantes, como a ***AUC-ROC***, que mede a capacidade do modelo de distinguir entre as classes, e a ***Precis√£o, Recall e F1-Score***, que nos d√£o uma vis√£o mais detalhada sobre o desempenho do modelo em cada classe (invas√£o de domic√≠lio e viol√™ncia f√≠sica)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208,
     "referenced_widgets": [
      "e56e82820d6343abbd761542e38d5fd6",
      "915e659cc2c1415eb570a678ca2378ef",
      "902c6c29a4f94c9492ad88e8acf835d7",
      "c99949dbb3d24348baa077692ae73aae",
      "b3d41c2939344fe59d0c0bb03759bc0e",
      "15c178daed0f4318ae8d8cdee1db2496",
      "7cbe081584d9436190b8cebab2bc5ebd",
      "ba48aff7a149407eab60731f7143f9d5",
      "556aa3c5ead0432290638ef9f93d0faa",
      "90bd0fb246244060bff4faab5c5add5b",
      "1102b1930f5941b981417794daf3f619",
      "d948e44aa8114c349b4bc649bd4d359f",
      "68fcad1f691043fb9f991176c32b008e",
      "e7f2193535804503abd8e3d50ef95035",
      "1c08187150aa46139952a3f6f23b58a9",
      "7a18f40bbdda41babdf4b41fe9950c73",
      "740d7f1c248d4719960f198c821f2446",
      "c1d7a6eb591e4b5db0088105e3da3eef",
      "78332e63a07d43f0b6282fb3659e097a",
      "1debb36355914cbcb6a7378f3df1dcbb",
      "39b12cf371e54a41ace40695b03ca5ab",
      "745387315ea74f6993a11ec7ac9ce49a",
      "ab916ac482884cbb8660c5ffc46743a7",
      "d3c17074f52b45a19b099301cf298dcd",
      "e308842fc8f940c5a399c487d89651ec",
      "f7f1434e74584c66ba2c5e19c4d9758a",
      "4a6e0ee5a36a4ade8c3618149196d61e",
      "33b55538e789410ba0b9b6ca2b04b5d9",
      "41a91186dd2a453b9a54c4a144dd8556",
      "e6b220ccd7504dfaa9f797c7851284f5",
      "fc9060df5ddd42c0a29efc3c569c552b",
      "8b6e69f309ff4cfb98b429010567887a",
      "fd57a96fea9c4fae934aa42932b97349",
      "b86b9f6118374a9e9f2eb87d734f44b6",
      "025514280c7c40dba7023e308cf69c23",
      "63753e7399684b1f97577372920b52a7",
      "19732009335a4a6e8acc82115f34bc3f",
      "2c89c064100c4d2c9a0f736e9095ab71",
      "3bb894f8416241ef85d97fe90b10a18e",
      "9fcb7c12c5534bddb1ca4c2134178c07",
      "5ebb86b9e5b64a2e92f8fd44821b39a9",
      "7eddc4934b694f02aae73ff92ed970ce",
      "d196c6c24cfe42d3a200b3ce8ec2a2b3",
      "bdbc41ad0a5643d49da3f95020fe3fd2",
      "52022b3ac30947b4bf6f4a0f6e052929",
      "7ef921d8f59840b38d41ef33c3611e24",
      "3521c48edad44e25b3deaeaa0154f11b",
      "c69982b847714e42b0c1c8c52e10fb51",
      "b3cf10b3331c4325b1598d581c4ee355",
      "7bb3ffb0d41a42f3943b2e46d3165a67",
      "8bb2991a65fc45df8ffdb46f761762d0",
      "e5d08ee0b7234edfafdb93d45074e557",
      "cdca1898657945db84651f72b2165f21",
      "325b4745eec94b44bb2ddac2775b0f7d",
      "a60950ee5d0d47378046544c0bd70422",
      "26dc67c6586e4d12944c2a6017012c74",
      "39dcc1025b02476597533a496f0b64d7",
      "6eeec8174cad41e4b0c1c419eab1da31",
      "f69a933f9b784503b28ae4e99a25b4fb",
      "94675df8437140f6bfe38f3732206f58",
      "99d88cf34cb2405c90fa42ab8a60abea",
      "32abd403fad74b22b1859d66cbbd613b",
      "6207e4bc3a2643f78ae6e2f8ef3f9dd4",
      "f016bd75cf7b41bb8a8020c698e6bedf",
      "fb3166a409e14659a76f57177522c890",
      "a2594b11e1a64d15b69a92a298192dc8"
     ]
    },
    "id": "F-gctrIbZ1Pv",
    "outputId": "8637d819-23a9-4792-b21e-dd549120b8c2"
   },
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "auc_score = evaluate.load(\"roc_auc\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        probabilities = np.exp(predictions - np.max(predictions, axis=-1, keepdims=True)) / np.sum(np.exp(predictions - np.max(predictions, axis=-1, keepdims=True)), axis=-1, keepdims=True)\n",
    "        positive_class_probs = probabilities[:, 1]\n",
    "\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "        acc = accuracy.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "        auc = auc_score.compute(prediction_scores=positive_class_probs, references=labels)[\"roc_auc\"]\n",
    "\n",
    "        f1_results = f1_metric.compute(\n",
    "            predictions=preds,\n",
    "            references=labels,\n",
    "            average=None,\n",
    "            labels=[label2id[\"invasao_domicilio\"], label2id[\"violencia_fisica\"]]\n",
    "        )\n",
    "        precision_results = precision.compute(\n",
    "            predictions=preds,\n",
    "            references=labels,\n",
    "            average=None,\n",
    "            labels=[label2id[\"invasao_domicilio\"], label2id[\"violencia_fisica\"]]\n",
    "        )\n",
    "        recall_results = recall.compute(\n",
    "            predictions=preds,\n",
    "            references=labels,\n",
    "            average=None,\n",
    "            labels=[label2id[\"invasao_domicilio\"], label2id[\"violencia_fisica\"]]\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"accuracy\": round(acc, 4),\n",
    "            \"auc\": round(auc, 4),\n",
    "            \"f1_invasao\": round(f1_results[\"f1\"][0], 4),\n",
    "            \"f1_violencia\": round(f1_results[\"f1\"][1], 4),\n",
    "            \"precision_invasao\": round(precision_results[\"precision\"][0], 4),\n",
    "            \"precision_violencia\": round(precision_results[\"precision\"][1], 4),\n",
    "            \"recall_invasao\": round(recall_results[\"recall\"][0], 4),\n",
    "            \"recall_violencia\": round(recall_results[\"recall\"][1], 4),\n",
    "            \"precision_macro\": round(np.mean(precision_results[\"precision\"]), 4),\n",
    "            \"recall_macro\": round(np.mean(recall_results[\"recall\"]), 4),\n",
    "            \"f1_macro\": round(np.mean(f1_results[\"f1\"]), 4)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no c√°lculo de m√©tricas: {str(e)}\")\n",
    "        return {\"accuracy\": 0.0, \"auc\": 0.0, \"f1_invasao\": 0.0, \"f1_violencia\": 0.0,\n",
    "                \"precision_invasao\": 0.0, \"precision_violencia\": 0.0,\n",
    "                \"recall_invasao\": 0.0, \"recall_violencia\": 0.0,\n",
    "                \"precision_macro\": 0.0, \"recall_macro\": 0.0, \"f1_macro\": 0.0}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fmwNJA-Z5DK"
   },
   "source": [
    "# **5. Treinamento: A m√°gica do Fine-Tuning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4ogB-Rqv6kG"
   },
   "source": [
    "Nesta se√ß√£o, iniciamos o treinamento do modelo usando a biblioteca ***Hugging Face Trainer***. O treinamento ser√° executado por 30 √©pocas. O ***EarlyStoppingCallback*** √© ativado para interromper o treinamento caso o desempenho do modelo no conjunto de valida√ß√£o pare de melhorar por tr√™s √©pocas consecutivas, evitando o overfitting. Voc√™ pode acompanhar a evolu√ß√£o da perda (loss) e da acur√°cia durante o processo no gr√°fico abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Lu5KpVfWaGA3",
    "outputId": "bad9f62e-42c9-4111-9557-8c07259c58f7"
   },
   "outputs": [],
   "source": [
    "train_losses_plot, val_losses_plot, test_losses_plot = [], [], []\n",
    "train_accuracies_plot, val_accuracies_plot, test_accuracies_plot = [], [], []\n",
    "\n",
    "# Configura√ß√£o dos argumentos de treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bertimbau-denuncias\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=30,\n",
    "    eval_strategy=\"epoch\",        # Avaliar no conjunto de valida√ß√£o ao final de cada √©poca\n",
    "    save_strategy=\"epoch\",        # Salvar checkpoint ao final de cada √©poca\n",
    "    logging_strategy=\"epoch\", # Registrar m√©tricas ao final de cada √©poca\n",
    "    load_best_model_at_end=True,  # Carregar o melhor modelo (baseado na m√©trica de valida√ß√£o) ao final do treino\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",             # Desativar report para plataformas como Weights & Biases\n",
    "    fp16=True,                    # Habilitar mixed precision training\n",
    "    seed=42,\n",
    "    gradient_accumulation_steps=1,\n",
    "    remove_unused_columns=True,\n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "# Data Collator para padding din√¢mico\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "# Custom Dataset (para trabalhar com DataFrames)\n",
    "class CustomDataset(TorchDataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        # Converte para tensor e garante que encodings.items() tem os valores corretos\n",
    "        self.encodings = {key: torch.tensor(val) for key, val in encodings.items()}\n",
    "        # Converte labels para tensor, lidando com pd.Series ou listas/arrays\n",
    "        self.labels = torch.tensor(labels.values if isinstance(labels, pd.Series) else labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = self.labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Criar datasets\n",
    "# Assegure que train_df, val_df, test_df e label2id estejam definidos.\n",
    "# .tolist() √© importante para garantir que labels sejam um tipo Python nativo antes de converter para tensor no CustomDataset.\n",
    "train_dataset = CustomDataset(train_encodings, train_df['classe'].map(label2id).tolist())\n",
    "val_dataset = CustomDataset(val_encodings, val_df['classe'].map(label2id).tolist())\n",
    "test_dataset = CustomDataset(test_encodings, test_df['classe'].map(label2id).tolist())\n",
    "\n",
    "# Instanciar o Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset, # O trainer usar√° este para avalia√ß√£o autom√°tica por √©poca\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold = 1e-3)],\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "try:\n",
    "    print(\"\\nüöÄ Iniciando treinamento...\")\n",
    "\n",
    "    # O trainer.train() ir√° executar o treinamento e as avalia√ß√µes de valida√ß√£o\n",
    "    # por √©poca, al√©m de carregar o melhor modelo no final.\n",
    "    train_results = trainer.train()\n",
    "\n",
    "    # --- Extrair m√©tricas dos logs para os gr√°ficos ---\n",
    "    # Os logs cont√™m as m√©tricas de treino e valida√ß√£o por √©poca\n",
    "    logs = trainer.state.log_history\n",
    "\n",
    "    # Iterar sobre os logs para preencher as listas de plotagem\n",
    "    for log_entry in logs:\n",
    "        # Perdas de Treino: o 'loss' √© logado para o treinamento\n",
    "        if 'loss' in log_entry and 'eval_loss' not in log_entry:\n",
    "            train_losses_plot.append(log_entry['loss'])\n",
    "\n",
    "        # M√©tricas de Valida√ß√£o: logadas com prefixo 'eval_'\n",
    "        if 'eval_loss' in log_entry and 'eval_accuracy' in log_entry:\n",
    "            val_losses_plot.append(log_entry['eval_loss'])\n",
    "            val_accuracies_plot.append(log_entry['eval_accuracy'])\n",
    "\n",
    "    # --- Avalia√ß√£o final do melhor modelo no conjunto de teste ---\n",
    "    # Este √© o ponto mais importante para as m√©tricas finais do seu modelo.\n",
    "    print(\"\\nüß™ Avalia√ß√£o final do melhor modelo no conjunto de teste...\")\n",
    "    final_test_metrics = trainer.evaluate(test_dataset)\n",
    "\n",
    "    # --- üìä Gr√°ficos de compara√ß√£o por √©poca ---\n",
    "\n",
    "    # O n√∫mero de √©pocas para plotagem ser√° o n√∫mero de avalia√ß√µes de valida√ß√£o registradas\n",
    "    epochs_completed = len(val_accuracies_plot)\n",
    "    epochs_to_plot = list(range(1, epochs_completed + 1))\n",
    "\n",
    "    train_losses_for_plot = train_losses_plot[-epochs_completed:] if len(train_losses_plot) >= epochs_completed else train_losses_plot\n",
    "\n",
    "    if not epochs_to_plot:\n",
    "        print(\"\\nN√£o h√° dados suficientes para gerar os gr√°ficos. Verifique o processo de treinamento e coleta de m√©tricas.\")\n",
    "    else:\n",
    "        # Loss por √©poca (Treino, Valida√ß√£o)\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(epochs_to_plot, train_losses_for_plot, label=\"Treino\", marker='o')\n",
    "        plt.plot(epochs_to_plot, val_losses_plot, label=\"Valida√ß√£o\", marker='o')\n",
    "        # Plotar o resultado final do teste como um ponto ou linha horizontal.\n",
    "        plt.xlabel(\"√âpoca\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Loss por √âpoca (Treino, Valida√ß√£o)\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during training or evaluation: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFSsNAWvaQQs"
   },
   "source": [
    "# **6. AVALIA√á√ÉO FINAL NO TESTE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "0a5gLzc6aQuU",
    "outputId": "290fc878-551b-47d0-fd69-5f695ff20bd4"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n==================== RESULTADOS FINAIS DO MODELO ====================\")\n",
    "print(\"\\nAvalia√ß√£o no conjunto de teste:\")\n",
    "\n",
    "test_results = trainer.predict(test_dataset)\n",
    "final_metrics = test_results.metrics\n",
    "\n",
    "for metric, value in final_metrics.items():\n",
    "    print(f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SLL_XiZaSle"
   },
   "source": [
    "# **7. SALVAR MODELO E DADOS DE RESULTADOS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbEnj8O5aUmx",
    "outputId": "edc55fca-4de4-4149-abd7-fbaea7126cdf"
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/content/drive/MyDrive/2025/tcc-final/resultados/modelo_bertimbau_final\")\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/2025/tcc-final/resultados/modelo_bertimbau_final\")\n",
    "print(\"Modelo salvo no Google Drive!\")\n",
    "\n",
    "# Salvar resultados do treinamento e r√≥tulos/previs√µes finais\n",
    "output_results_path = \"/content/drive/MyDrive/2025/tcc-final/resultados/resultados_fine_tuning.xlsx\"\n",
    "\n",
    "# Get the predictions from the test_results object\n",
    "predictions = np.argmax(test_results.predictions, axis=1)\n",
    "\n",
    "\n",
    "# Criar um DataFrame para os resultados de teste\n",
    "results_df = pd.DataFrame({\n",
    "    'texto': test_df['texto'],\n",
    "    'rotulo_verdadeiro': test_df['classe'],\n",
    "    'rotulo_predito_id': predictions,\n",
    "    'rotulo_predito': [id2label[p] for p in predictions]\n",
    "})\n",
    "\n",
    "# Salvar o DataFrame de resultados em um arquivo Excel\n",
    "results_df.to_excel(output_results_path, index=False)\n",
    "print(f\"Resultados de teste (texto, r√≥tulo verdadeiro, r√≥tulo predito) salvos em: {output_results_path}\")\n",
    "\n",
    "# Salvar as m√©tricas finais em um arquivo de texto ou CSV\n",
    "metrics_output_path = \"/content/drive/MyDrive/2025/tcc-final/resultados/metricas_finais.txt\"\n",
    "with open(metrics_output_path, 'w') as f:\n",
    "    f.write(\"M√©tricas Finais da Avalia√ß√£o no Conjunto de Teste:\\n\")\n",
    "    for metric_name, value in final_metrics.items():\n",
    "        f.write(f\"{metric_name}: {value:.4f}\\n\")\n",
    "print(f\"M√©tricas finais salvas em: {metrics_output_path}\")\n",
    "\n",
    "# --- FIM DO CONTADOR DE TEMPO ---\n",
    "end_total_time = time.time()\n",
    "total_execution_time = end_total_time - start_total_time\n",
    "print(f\"\\nTempo total de execu√ß√£o do script: {total_execution_time:.2f} segundos\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
