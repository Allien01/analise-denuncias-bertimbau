{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsCCzLwYbKDF"
   },
   "source": [
    "@inproceedings{souza2020bertimbau, author = {F{'a}bio Souza and Rodrigo Nogueira and Roberto Lotufo}, title = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese}, booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)}, year = {2020} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VU_ldpdMEMbw",
    "outputId": "7c8fb600-6dc4-4e43-ab0e-4a1acf20e512"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BkkHsJY5z-5",
    "outputId": "3ecd0da8-f179-4d5e-deec-285a5a34c506"
   },
   "outputs": [],
   "source": [
    "!pip install nbstripout\n",
    "!nbstripout /content/drive/MyDrive/2025/tcc-final/fine_tuning_Bertimbau_version2_24_07.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feHRGfqdONsa",
    "outputId": "ae484afd-2aff-4e38-fccf-56c47e9cd8b2"
   },
   "outputs": [],
   "source": [
    "!pip install transformers evaluate accelerate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback # Importa explicitamente para o callback\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import evaluate\n",
    "start_total_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkRUcLU1PHke"
   },
   "source": [
    "# **1. Prepara√ß√£o do Dataset para K-Fold**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG-9Pr_Mw3jU"
   },
   "source": [
    "Iniciamos carregando o dataset completo de den√∫ncias. Diferente do m√©todo anterior, aqui n√£o faremos uma divis√£o fixa. Em vez disso, usaremos a ***t√©cnica de Valida√ß√£o Cruzada (K-Fold)***, que nos permite treinar e validar o modelo v√°rias vezes em subconjuntos diferentes do nosso dataset, obtendo uma avalia√ß√£o de desempenho mais confi√°vel e robusta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3O3x60E3PHtT",
    "outputId": "9a1f2f99-a119-42eb-f29d-4996e9bdb512"
   },
   "outputs": [],
   "source": [
    "# Carregar dataset j√° processado\n",
    "file_path = '/content/drive/MyDrive/2025/tcc-final/denuncias_balanceadas.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(f\"Dataset carregado. Total de registros: {len(df)}\")\n",
    "print(\"Primeiras 5 linhas do dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\nInforma√ß√µes do dataset:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIeftoByY356"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE3WAEgOPYHa"
   },
   "source": [
    "# **2. Processando o Texto com o BERTimbau**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoC0YTBEYuA6"
   },
   "source": [
    "Assim como na abordagem anterior, esta etapa √© crucial para preparar o texto. Usamos o ***tokenizer*** do modelo ***BERTimbau*** para converter as den√∫ncias em um formato num√©rico compreens√≠vel pelo modelo. Tamb√©m criamos classes personalizadas ***(CustomDataset)*** e um *** DataCollator*** para otimizar a forma como os dados s√£o alimentados ao modelo durante o treinamento e a avalia√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275,
     "referenced_widgets": [
      "72c6f50353584e5485d6b6c914483257",
      "6fdebdf6bdc4478599bafe0fc631b9d1",
      "011d4c5b9614462b99fc433caaf96c50",
      "bf57037d91ef4b5e93ad42aae518472f",
      "e4799f03869344c1ab9dfffa6b615c48",
      "552a19dc6a2b435991f170bb1ee40a87",
      "91ac840cea1741b7a22fe946e9afd76d",
      "c85efe8988964d01a95bf61ce9f01924",
      "9894277dda974db69fbb8493ca135e19",
      "6cf9873e0bd54cf1b3b95b43c30ddcab",
      "c9e7c729913c4a17aedbae26632bd803",
      "943049a28a4e47509afb633d84e49a28",
      "c9b3ea282f3e4f6e94a9c4c2d22c1706",
      "d318799422e7456aabcf5fff852a0f63",
      "cd3f0e4cfd9a45cbaef6d34502da7eca",
      "7dbd7e814e55458f8d941924b09bfedc",
      "879e70d1c0e141b8ba71185e53e63e4e",
      "919f8290f23e408e8d134be731c55a3e",
      "41cf71f421824ba88d5decbc32052ab6",
      "9350ec8b3eb04e0babc13f2025bcfb7b",
      "a674feafbeb14c72a98d690469b42186",
      "f2a3e14c5a2048f4b7cc604c5164183a",
      "2a63b27644f84e31a49570657ad6a3bc",
      "cbe422ff7c2643edb148e7bf51013556",
      "6afe6a1185c84e9686ec43a3dda417f8",
      "a2b91d57b2a54586b9323fd53caa3a67",
      "aa2b5b2434d14b179c33249d06522c77",
      "e594cca9db594687b885efd344c1b2ff",
      "68e99747ef914c0c915c2622b15d6c73",
      "8772c0d7dbac466f8c8f81db11b31e4d",
      "f7cc7fdbd3e6451c9e973ff8703330e4",
      "1fcf542184774831ab4d46e6c6ebf3a3",
      "101ca64945294ec0bc8cc12e448b5fe7",
      "d9496f10c93b4af7a0a87ea501051a1d",
      "b336507ee29249d59008f7e67f1e65fc",
      "e306bb03003e4c63ac21ad6d46630226",
      "06c0a5e77f8a4d9abe2b73e720ad17de",
      "65ebac71646e451f8a9b69ed30190512",
      "0599f86f87f6499682d0462556294ef7",
      "dbca2902a9924fbab4f03c45db2d31b3",
      "d5dc1cae3c874473b744aa189a82ade7",
      "7d667446f33949e59ddefb7c78256352",
      "7dd5575be2ff4680a72b7f4ba8803384",
      "42bfc40f310a4780bf4f8eeb9c6e5f58",
      "59b57bd745aa46fca161dcc24ebedb60",
      "6c46e93805334a7e86f1ba090d131d96",
      "fdfc1645e0204a3cb98d08705c231d51",
      "b57d3566cf9044acb459e0e8848e0070",
      "f1a952c66e7a41268b033a19a94939ab",
      "8cd6910cca054ba2971c668ce479c3c2",
      "3ccd601b00f04c4e89ef271a6bdf4ec8",
      "108518d4d3a846cfb7694a8afd483c49",
      "2c39e7c2c14343dd9d06cabdffda169b",
      "bc1a2e1a7cae4cd6a09d75ab21d649c1",
      "dea38a6f6d20405fab6e0be58dd7299f"
     ]
    },
    "id": "sql7RSXMPd1g",
    "outputId": "f46cdac7-49ef-4e65-b2e9-0a4c64bbe6b9"
   },
   "outputs": [],
   "source": [
    "model_path = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"texto\"], truncation=True, max_length=512, padding='longest', return_tensors='pt')\n",
    "\n",
    "# Mapeamento de Labels: Essencial para o modelo e m√©tricas\n",
    "id2label = {0: \"invasao_domicilio\", 1: \"violencia_fisica\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Custom Dataset (para trabalhar com tensores PyTorch diretamente)\n",
    "class CustomDataset(TorchDataset): # Certifique-se que 'Dataset' foi importado corretamente\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings # Agora ser√° um dicion√°rio de tensores (input_ids, attention_mask)\n",
    "        self.labels = labels       # Agora ser√° uma lista de labels (ou Series com √≠ndice reiniciado)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # Acesso direto pelo √≠ndice\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Data Collator para padding din√¢mico durante o treinamento\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S-nz2HQZ0CA"
   },
   "source": [
    "# **3. M√©tricas de Desempenho**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIMdebFsx3td"
   },
   "source": [
    "Para avaliar o desempenho do nosso modelo de forma completa, definimos uma s√©rie de m√©tricas. Al√©m da ***acur√°cia*** (a propor√ß√£o de acertos), calculamos o ***AUC-ROC*** (que mede a capacidade de distin√ß√£o entre as classes), e as m√©tricas de ***Precis√£o, Recall e F1-Score***. O c√°lculo dessas m√©tricas para cada classe e a m√©dia (***macro***) nos d√£o uma vis√£o detalhada do desempenho do modelo, especialmente em datasets com classes desbalanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176,
     "referenced_widgets": [
      "4f6eefc075944575954b50090233c146",
      "8f3de3def527458380a0efd611b3bd41",
      "85cdbedc03404a88a048c000a15235b8",
      "3aa06ec94ffa43bcafea407376da93a6",
      "beb34470789f42cdbf9aaa40a9803849",
      "8b070cac2047435a8d220db5f7b11700",
      "4a25afb2b868464fbc7526f5571a5d49",
      "c37d9859956c4700a82baa265a4461ba",
      "af47394487f344238853dd3d55b757d3",
      "00778e52d0894f9ba976077951aef426",
      "1a5730674f154b11a37e3cafa6f95afd",
      "e15d032558124f6289a50da49e169556",
      "85e68c15c8614c539b805a8a27c47b40",
      "bce169cf6f6f420e8d71bfe99523e6c8",
      "2812d0f27aa94ad6b70837336ed0954a",
      "e13c3887957d4af6a3148501bbd021fb",
      "1868160d8c8c4a5b8a5b7d7c6da28399",
      "392174895e0a42c9a1f63f3fb9de0266",
      "39f578749bb846e282b7eeebc9e1f623",
      "eccfd3ba9ee545e4b1d2f9fbe4791152",
      "7b8d06a59b5c496bbf1e46e87983c88b",
      "7d8713cac22a431e8b4558ecb182bc8d",
      "d1ed58242f814286b5c7b99979ed3574",
      "7dce18c403fc4ed89aa5dd4c9c49816d",
      "578a065e04b24ae28b42dae0f2a993b4",
      "278743ce85c44d55a3b0f2b850414d72",
      "a17921b107d24273b84d6403bed2d59e",
      "4b46b91c938c4a1a949818d41a4d08f7",
      "5c86e3692b73436bb22e50f05ed08037",
      "6c56f7ba62eb4fa7a4f0d1e01495f472",
      "bdf6f10018134f76aedcbc7048e6a1df",
      "aba435749db14c038da049b0ef407e00",
      "b6b187d7d83c49d0985b73abe931c712",
      "dd3be8897b55453b9da896fa27b9012c",
      "d41858e0fe634f0c9cf644b3c0b8524a",
      "2b13b6926fe7462aaf50f85612a4fdf0",
      "2e56990bb7314e47974a42d7434d2f86",
      "5e733d7d54e14d36a89354996e075c44",
      "2e0b8f975ddd4f7bb1369b5712300c2e",
      "dc80d8d22d994c3c92d4ba06a8e477a4",
      "84e5792e050c408c9b533e9b8331d3dd",
      "7594801b9e2244259270c7d074a6755d",
      "f298f6f503b54c5db2be112c836edae9",
      "a89dc89d864a4c4e84cbb082fc26ee10",
      "1690becbae9c49f088b4bf0fdf6f8f1a",
      "3def3b0a61d246f1aa54b92ab88703bf",
      "2452ec64a4e44d7ba380142d73265318",
      "7ab33d34498244f386a5dd96494942df",
      "a98be5e4ed614731941d163bcab268ac",
      "ba857004751d4ebba0c1c9108ba3564d",
      "be86885dc2444b7985b5b3dab058b6e5",
      "c9cec8a7ed774c54a506217bd6b491ba",
      "c4903ace7bcd4d4498b21388d3bc9ed3",
      "5019c894cf1c4c9b8ef66486f04125ec",
      "30909acb614847fca0ff8df5232d442c"
     ]
    },
    "id": "F-gctrIbZ1Pv",
    "outputId": "482b98e4-5c55-416d-c485-101f714456c7"
   },
   "outputs": [],
   "source": [
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "auc_metric = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        probabilities = np.exp(predictions - np.max(predictions, axis=-1, keepdims=True)) / np.sum(np.exp(predictions - np.max(predictions, axis=-1, keepdims=True)), axis=-1, keepdims=True)\n",
    "        positive_class_probs = probabilities[:, 1]\n",
    "\n",
    "        # Classe prevista (ID da maior probabilidade)\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Labels das classes, conforme definido no notebook\n",
    "        class_labels = [label2id[\"invasao_domicilio\"], label2id[\"violencia_fisica\"]]\n",
    "\n",
    "        # --- C√ÅLCULO DAS M√âTRICAS ---\n",
    "        acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "        auc = auc_metric.compute(prediction_scores=positive_class_probs, references=labels)[\"roc_auc\"]\n",
    "\n",
    "        f1_results_by_class = f1_metric.compute(predictions=preds, references=labels, average=None, labels=class_labels)[\"f1\"]\n",
    "        f1_macro = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "\n",
    "        precision_results_by_class = precision_metric.compute(predictions=preds, references=labels, average=None, labels=class_labels)[\"precision\"]\n",
    "        precision_macro = precision_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"]\n",
    "\n",
    "        recall_results_by_class = recall_metric.compute(predictions=preds, references=labels, average=None, labels=class_labels)[\"recall\"]\n",
    "        recall_macro = recall_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"]\n",
    "\n",
    "        # --- RETORNO DO DICION√ÅRIO COMPLETO DE M√âTRICAS ---\n",
    "        return {\n",
    "            \"accuracy\": round(acc, 4),\n",
    "            \"auc\": round(auc, 4),\n",
    "            \"f1_invasao\": round(f1_results_by_class[0], 4),\n",
    "            \"f1_violencia\": round(f1_results_by_class[1], 4),\n",
    "            \"precision_invasao\": round(precision_results_by_class[0], 4),\n",
    "            \"precision_violencia\": round(precision_results_by_class[1], 4),\n",
    "            \"recall_invasao\": round(recall_results_by_class[0], 4),\n",
    "            \"recall_violencia\": round(recall_results_by_class[1], 4),\n",
    "            \"precision_macro\": round(precision_macro, 4),\n",
    "            \"recall_macro\": round(recall_macro, 4),\n",
    "            \"f1_macro\": round(f1_macro, 4)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no c√°lculo de m√©tricas: {str(e)}\")\n",
    "        # Em caso de erro, retorna valores padr√£o para n√£o quebrar o treinamento\n",
    "        return {\n",
    "            \"accuracy\": 0.0,\n",
    "            \"auc\": 0.0,\n",
    "            \"f1_invasao\": 0.0,\n",
    "            \"f1_violencia\": 0.0,\n",
    "            \"precision_invasao\": 0.0,\n",
    "            \"precision_violencia\": 0.0,\n",
    "            \"recall_invasao\": 0.0,\n",
    "            \"recall_violencia\": 0.0,\n",
    "            \"precision_macro\": 0.0,\n",
    "            \"recall_macro\": 0.0,\n",
    "            \"f1_macro\": 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fmwNJA-Z5DK"
   },
   "source": [
    "# **4. Valida√ß√£o Cruzada (K-Fold) com Early Stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwxxmwBqyXXV"
   },
   "source": [
    "Esta √© a etapa central do nosso projeto. Utilizamos a ***Valida√ß√£o Cruzada com 5 folds*** para treinar o modelo. Isso significa que o dataset √© dividido em 5 partes: o modelo √© treinado em 4 delas e validado na 5¬™. Este processo se repete 5 vezes, garantindo que todas as den√∫ncias sejam usadas na valida√ß√£o e nos dando uma estimativa mais precisa do desempenho geral do modelo. O ***Early Stopping*** √© uma ferramenta crucial para evitar o overfitting, interrompendo o treinamento se o modelo parar de melhorar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4f247199f0c44f9aa8f4d1b8d935d68f",
      "86387217fc97417e91941a7d0da94554",
      "3eeb0a7f19c84b1fb28d3ab40c035aac",
      "c4a13d48229f453991f267c2991ea713",
      "63d3caee45084085aba016c24f8c4fa8",
      "dd1179807f3f4b97828cf7aafae48a71",
      "7aedd7711b084dadb7e818b9c7b2c80c",
      "ead218a2631341aa842b6ec0228c84f5",
      "50b542de62db445c9e3fdd28e297a5ca",
      "58eb3ab898e44abfa5e8c8a1a68a637b",
      "1de6926edb2d42a389e8eeddbbdb61f4",
      "cc2bec15a7c14cd0bf99d8e2952700ad",
      "fff97fcbf70c4a31b28b01782b49a7e0",
      "2b72450c772449d1bbec457001f0d8a0",
      "e8d799f488e749cda0c3fd6ffd152a7e",
      "2489ef929a724e61a04b5dfe963cc475",
      "15d99fce86784a7297612736e4d52697",
      "a636f7adf4e342ffa92b944f1c3cbb49",
      "ab271997377b4466854a08fcdd23b84b",
      "87a2982e2c054746ab1fd2cbeda4334b",
      "82ab37c9b5c048798bcf720f29ab488e",
      "0036ae7dabe6421da47eb96399332f6f"
     ]
    },
    "id": "Vf6OgrwXH9QE",
    "outputId": "1a29b309-5f56-42cd-9295-7d4cd27e8f70"
   },
   "outputs": [],
   "source": [
    "# Essas listas armazenar√£o o hist√≥rico de m√©tricas de CADA fold para a plotagem.\n",
    "all_fold_train_losses = []\n",
    "all_fold_eval_losses = []\n",
    "all_fold_eval_accuracies = []\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Par√¢metros de Treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bertimbau-denuncias-cv\", # Diret√≥rio de sa√≠da para K-Fold\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=3e-5,\n",
    "    num_train_epochs=30, # Aumentado para permitir que Early Stopping atue\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\", # Monitorar a perda de valida√ß√£o\n",
    "    greater_is_better=False, # Menor perda √© melhor\n",
    "    logging_dir=\"./logs-cv\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True, # Treinamento com precis√£o mista\n",
    "    seed=42, # Semente para reprodutibilidade\n",
    "    gradient_accumulation_steps=1,\n",
    "    remove_unused_columns=True,\n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "# Preparar dados para K-Fold\n",
    "all_texts = df['texto'].tolist()\n",
    "all_labels_mapped = df['classe'].map(label2id).tolist() # Labels j√° mapeadas para IDs num√©ricos\n",
    "\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# Lista para armazenar as m√©tricas de teste de cada fold\n",
    "all_fold_test_metrics = []\n",
    "print(f\"\\nüöÄ Iniciando Treinamento com K-Fold Cross-Validation ({N_SPLITS} folds)....\\n\")\n",
    "\n",
    "# Loop atrav√©s dos folds\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(all_texts, all_labels_mapped)):\n",
    "    print(f\"\\n==================== INICIANDO FOLD {fold+1}/{N_SPLITS} ====================\")\n",
    "\n",
    "    # Dividir os dados para o fold atual\n",
    "    train_fold_texts = [all_texts[i] for i in train_index]\n",
    "    train_fold_labels = [all_labels_mapped[i] for i in train_index]\n",
    "    test_fold_texts = [all_texts[i] for i in test_index]\n",
    "    test_fold_labels = [all_labels_mapped[i] for i in test_index]\n",
    "\n",
    "    # Tokenizar os dados para o fold atual\n",
    "    train_fold_encodings = tokenize_function({\"texto\": train_fold_texts})\n",
    "    test_fold_encodings = tokenize_function({\"texto\": test_fold_texts})\n",
    "\n",
    "    # Criar CustomDatasets para o fold atual\n",
    "    train_dataset = CustomDataset(train_fold_encodings, train_fold_labels)\n",
    "    eval_dataset_for_trainer = CustomDataset(test_fold_encodings, test_fold_labels)\n",
    "\n",
    "    # Re-instanciar o modelo para cada fold\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=2,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    # Congelar camadas do BERT (exceto pooler e classificador)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' not in name and 'pooler' not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    print(f\"‚öôÔ∏è N√∫mero de par√¢metros trein√°veis no FOLD {fold+1}: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    # Re-instanciar o Trainer para cada fold\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset_for_trainer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=1e-3)],\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Treinar o modelo para o fold atual\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "\n",
    "        # Salvar o best model do fold atual\n",
    "        output_dir_fold = f\"/content/drive/MyDrive/2025/tcc-final/resultados_kfold/fold-{fold+1}\"\n",
    "        trainer.save_model(output_dir_fold)\n",
    "        print(f\"‚úÖ Modelo salvo em: {output_dir_fold}\")\n",
    "\n",
    "        # Perform evaluation on the test set\n",
    "        evaluation_output = trainer.evaluate(eval_dataset_for_trainer)\n",
    "\n",
    "        # Extract predictions and true labels\n",
    "        predictions = trainer.predict(eval_dataset_for_trainer).predictions\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        true_labels = eval_dataset_for_trainer.labels\n",
    "\n",
    "        # Map predicted labels back to original class names\n",
    "        predicted_class_names = [id2label[label] for label in predicted_labels]\n",
    "        true_class_names = [id2label[label] for label in true_labels]\n",
    "\n",
    "        # Create a DataFrame with predictions\n",
    "        predictions_df = pd.DataFrame({\n",
    "            \"texto\": test_fold_texts,\n",
    "            \"classe_verdadeira\": true_class_names,\n",
    "            \"classe_prevista\": predicted_class_names\n",
    "        })\n",
    "\n",
    "        # Save predictions to Excel\n",
    "        predictions_output_path = f\"/content/drive/MyDrive/2025/tcc-final/resultados_kfold/fold_{fold+1}_predictions.xlsx\"\n",
    "        predictions_df.to_excel(predictions_output_path, index=False)\n",
    "        print(f\"‚úÖ Previs√µes salvas em: {predictions_output_path}\")\n",
    "\n",
    "        # --- L√ìGICA SIMPLIFICADA DE COLETA DE M√âTRICAS ---\n",
    "        logs = trainer.state.log_history\n",
    "\n",
    "        # Listas tempor√°rias para as m√©tricas deste fold\n",
    "        fold_train_losses = []\n",
    "        fold_eval_losses = []\n",
    "        fold_eval_accuracies = []\n",
    "\n",
    "        # Agora √© simples: 1 log por √©poca para cada m√©trica\n",
    "        for log_entry in logs:\n",
    "            if 'loss' in log_entry and 'eval_loss' not in log_entry:\n",
    "                fold_train_losses.append(log_entry['loss'])\n",
    "            if 'eval_loss' in log_entry:\n",
    "                fold_eval_losses.append(log_entry['eval_loss'])\n",
    "                fold_eval_accuracies.append(log_entry['eval_accuracy'])\n",
    "\n",
    "        # üîç DIAGN√ìSTICO: Verificar se est√° tudo alinhado\n",
    "        print(f\"\\nüìä FOLD {fold+1} - M√©tricas coletadas:\")\n",
    "        print(f\"   ‚Ä¢ Train losses: {len(fold_train_losses)} √©pocas\")\n",
    "        print(f\"   ‚Ä¢ Eval losses: {len(fold_eval_losses)} √©pocas\")\n",
    "        print(f\"   ‚Ä¢ Eval accuracies: {len(fold_eval_accuracies)} √©pocas\")\n",
    "\n",
    "        # ‚ö†Ô∏è SEGURAN√áA: Garantir mesmo tamanho (caso haja alguma inconsist√™ncia)\n",
    "        min_length = min(len(fold_train_losses), len(fold_eval_losses))\n",
    "        if len(fold_train_losses) != len(fold_eval_losses):\n",
    "            print(f\"   ‚ö†Ô∏è AVISO: Tamanhos diferentes! Ajustando para {min_length} √©pocas\")\n",
    "            fold_train_losses = fold_train_losses[:min_length]\n",
    "            fold_eval_losses = fold_eval_losses[:min_length]\n",
    "            fold_eval_accuracies = fold_eval_accuracies[:min_length]\n",
    "\n",
    "        # Armazenar as curvas de m√©tricas deste fold\n",
    "        all_fold_train_losses.append(fold_train_losses)\n",
    "        all_fold_eval_losses.append(fold_eval_losses)\n",
    "        all_fold_eval_accuracies.append(fold_eval_accuracies)\n",
    "        # --------------------------------------------------------\n",
    "\n",
    "        print(f\"\\nüìä M√©tricas finais de treino para FOLD {fold+1}:\")\n",
    "        print(f\"   Loss: {train_result.metrics.get('train_loss', 'N/A'):.4f}\")\n",
    "        print(f\"   Tempo total: {train_result.metrics.get('train_runtime', 'N/A'):.2f}s\")\n",
    "\n",
    "        # Evaluate on test set\n",
    "        print(f\"üß™ Avalia√ß√£o no conjunto de teste/valida√ß√£o do FOLD {fold+1}...\")\n",
    "        fold_test_metrics = trainer.evaluate(eval_dataset_for_trainer)\n",
    "        all_fold_test_metrics.append(fold_test_metrics)\n",
    "        print(f\"   Resultados do FOLD {fold+1}: {fold_test_metrics}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"‚ùå Erro durante o treinamento do FOLD {fold+1}:\")\n",
    "        print(traceback.format_exc())\n",
    "        all_fold_test_metrics.append({\n",
    "            'eval_loss': float('nan'),\n",
    "            'eval_accuracy': 0.0,\n",
    "            'eval_auc': 0.0,\n",
    "            'eval_f1_invasao': 0.0,\n",
    "            'eval_f1_violencia': 0.0,\n",
    "            'eval_runtime': float('nan'),\n",
    "            'eval_samples_per_second': 0.0,\n",
    "            'eval_steps_per_second': 0.0\n",
    "        })\n",
    "\n",
    "# --- C√ìDIGO DE PLOTAGEM SIMPLIFICADO ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# --- Gr√°fico 1: Perda de Treino por √âpoca ---\n",
    "for i, train_loss_history in enumerate(all_fold_train_losses):\n",
    "    epochs = range(1, len(train_loss_history) + 1)\n",
    "    ax1.plot(epochs, train_loss_history,\n",
    "             marker='x', linestyle='--', label=f'Fold {i+1}')\n",
    "\n",
    "ax1.set_xlabel(\"√âpoca\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Perda de Treino por √âpoca (Todos os Folds)\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# --- Gr√°fico 2: Perda de Valida√ß√£o por √âpoca ---\n",
    "for i, eval_loss_history in enumerate(all_fold_eval_losses):\n",
    "    epochs = range(1, len(eval_loss_history) + 1)\n",
    "    ax2.plot(epochs, eval_loss_history,\n",
    "             marker='o', linestyle='-', label=f'Fold {i+1}')\n",
    "\n",
    "ax2.set_xlabel(\"√âpoca\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_title(\"Perda de Valida√ß√£o por √âpoca (Todos os Folds)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFSsNAWvaQQs"
   },
   "source": [
    "# **5. RESULTADOS FINAIS DA VALIDA√á√ÉO CRUZADA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "0a5gLzc6aQuU",
    "outputId": "6310cdd5-f214-415f-c278-998e45cd2824"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n==================== RESULTADOS FINAIS K-FOLD CROSS-VALIDATION ====================\")\n",
    "\n",
    "if all_fold_test_metrics:\n",
    "    # Coletar todas as m√©tricas de todos os folds\n",
    "    df_metrics = pd.DataFrame(all_fold_test_metrics)\n",
    "\n",
    "    # Calcular m√©dias e desvios padr√£o\n",
    "    avg_metrics = df_metrics.mean(numeric_only=True).round(4).to_dict()\n",
    "    std_metrics = df_metrics.std(numeric_only=True).round(4).to_dict()\n",
    "\n",
    "    print(\"M√©tricas M√©dias (e Desvio Padr√£o) em todos os Folds:\")\n",
    "    for metric, avg_value in avg_metrics.items():\n",
    "        # Excluir m√©tricas de tempo/desempenho por segundo da apresenta√ß√£o principal se desejar\n",
    "        if 'runtime' not in metric and 'samples_per_second' not in metric and 'steps_per_second' not in metric:\n",
    "            print(f\"{metric}: {avg_value} (¬± {std_metrics.get(metric, 0)})\")\n",
    "else:\n",
    "    print(\"Nenhum resultado de fold foi coletado. Ocorreram erros em todos os folds.\")\n",
    "\n",
    "print(\"\\n‚úÖ K-Fold Cross-Validation Conclu√≠do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SLL_XiZaSle"
   },
   "source": [
    "# **6. SALVAR MODELO (Considera√ß√µes P√≥s K-Fold)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "PbEnj8O5aUmx",
    "outputId": "3e34969c-0dc1-4696-84c3-0effaa1393de"
   },
   "outputs": [],
   "source": [
    "criterio = \"eval_accuracy\"  # ou \"eval_auc\", \"eval_f1_invasao\", etc.\n",
    "best_fold_idx = int(np.argmax([m.get(criterio, float('-inf')) for m in all_fold_test_metrics]))\n",
    "print(f\"üèÜ Melhor modelo: Fold {best_fold_idx+1} com {criterio} = {all_fold_test_metrics[best_fold_idx][criterio]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
