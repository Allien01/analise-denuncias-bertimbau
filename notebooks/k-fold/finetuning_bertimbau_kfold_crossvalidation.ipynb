{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsCCzLwYbKDF"
   },
   "source": [
    "@inproceedings{souza2020bertimbau, author = {F{'a}bio Souza and Rodrigo Nogueira and Roberto Lotufo}, title = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese}, booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)}, year = {2020} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VU_ldpdMEMbw",
    "outputId": "637325ee-ac4e-4845-9a98-aad3f4d7ab25"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BkkHsJY5z-5",
    "outputId": "1c7fd651-da30-4a13-b8de-cfe0c20ea47f"
   },
   "outputs": [],
   "source": [
    "!pip install nbstripout\n",
    "!nbstripout /content/drive/MyDrive/2025/tcc-final/fine_tuning_Bertimbau_version2_24_07.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feHRGfqdONsa",
    "outputId": "9b9cec83-801a-4523-ea4d-f71083315344"
   },
   "outputs": [],
   "source": [
    "!pip install transformers evaluate accelerate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback # Importa explicitamente para o callback\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import evaluate\n",
    "start_total_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkRUcLU1PHke"
   },
   "source": [
    "# **1. Prepara√ß√£o do Dataset para K-Fold**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG-9Pr_Mw3jU"
   },
   "source": [
    "Iniciamos carregando o dataset completo de den√∫ncias. Diferente do m√©todo anterior, aqui n√£o faremos uma divis√£o fixa. Em vez disso, usaremos a ***t√©cnica de Valida√ß√£o Cruzada (K-Fold)***, que nos permite treinar e validar o modelo v√°rias vezes em subconjuntos diferentes do nosso dataset, obtendo uma avalia√ß√£o de desempenho mais confi√°vel e robusta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3O3x60E3PHtT",
    "outputId": "5a48c97e-5c3b-4888-dfd3-67defdedbb69"
   },
   "outputs": [],
   "source": [
    "# Carregar dataset j√° processado\n",
    "file_path = '/content/drive/MyDrive/2025/tcc-final/denuncias_balanceadas.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(f\"Dataset carregado. Total de registros: {len(df)}\")\n",
    "print(\"Primeiras 5 linhas do dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\nInforma√ß√µes do dataset:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIeftoByY356"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE3WAEgOPYHa"
   },
   "source": [
    "# **2. Processando o Texto com o BERTimbau**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoC0YTBEYuA6"
   },
   "source": [
    "Assim como na abordagem anterior, esta etapa √© crucial para preparar o texto. Usamos o ***tokenizer*** do modelo ***BERTimbau*** para converter as den√∫ncias em um formato num√©rico compreens√≠vel pelo modelo. Tamb√©m criamos classes personalizadas ***(CustomDataset)*** e um *** DataCollator*** para otimizar a forma como os dados s√£o alimentados ao modelo durante o treinamento e a avalia√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275,
     "referenced_widgets": [
      "0f7eaceac51e486ca84d4ad13d808a20",
      "000d85abea9a4259b4518cae1fcf0478",
      "2c8a139dffed48649d15e84beaa7df2e",
      "ead16d03d90b44f38f75f4c2adf55d01",
      "b27cc9fbf5774151952cc2dce39b84d1",
      "ce396e625ece48a0b89af7046b0754e2",
      "84f6bf6526fa4861b5ebd0e610001bd3",
      "03ecaddd62ad4442acad9a36742b8604",
      "cc6308cfbe2a4091957b00946639b331",
      "8527d87b80384a7fb0ae79e618ded632",
      "d00dc0a9aeba450d9bbd02034b67c0a4",
      "87ffba87c06745d08d78fdb4ce907dfa",
      "2be10ed760a0488cb58d3aa6a38d1813",
      "348d914fc8544aaf8ed46a4aae79ae1e",
      "878e21b44c2449db974985c4f3b7f423",
      "dca00e5d386f483cbcd00c37b7fea5b2",
      "41e6f825ec6e47339fab344268de0d59",
      "4f33a29bf3a64094afe93dc5b570f588",
      "4a803ce402f842a0bd526a049d8a816d",
      "d1ab3ee62cb64f4c8f3931a3793b35c5",
      "42fae270900c4e3e85211a2ed1ae56a2",
      "a09785c2a9524e979e6e818fe866bb4b",
      "72b31b7f797342f385557c3ed8c21122",
      "94d09f224adc4f22839dc6d57153ea94",
      "1284b1997dc34dbc9571d9d833d5b838",
      "0e7c80a4977a4740a333b1451303ff8c",
      "d69110ff77ed421cbd30259ee13bc434",
      "a109bd675ffd4cd2b4b661ba897484ca",
      "01a9d3c8009e4df1be185f48cb71a7d1",
      "67f510be907e4fb89612fb62d3cabfb8",
      "882f7f1523ee4e70a41f78b353982ff1",
      "48a02817340b4be7a43eacbe5102b38b",
      "be543148b152401b8ad76946a87d9c15",
      "5a3264fbd6e94b41b0b2ed65ed481c44",
      "4a41ac185263426583174b19b006efb4",
      "10504be14c274a26b8cda1a491717cc1",
      "d6db59bbfdfe42bfb76a563b04e7e5d8",
      "bfbc5a93fc254c6fb7fbfcdd358ebe52",
      "09e99a5b9da542b18d76f9bf460a54ab",
      "01720843940b45efa97b206b9a8949d3",
      "a34dd892c10e4ee1a5659c491becf5d0",
      "98f390524ed542e1b20bd2d7f95ad759",
      "bc7a19e0a51d49bcace16afdf422dd8a",
      "77eb03c7606b4d4198756ad350da7f61",
      "4fd2abeb7be145eaac348c766cd8feaf",
      "c75687cb485048cbad44c43f934bd31d",
      "f17ae74f604148c5b11f8efeece56f6e",
      "7aa54996f15e40c283604646d766cd57",
      "3f54ea1c50de4521ba96d9c833ea3c6c",
      "767d58dba93a44aea0cdc7f5aee069c9",
      "1bb13dfc377e4f8f856563ec0d435ffb",
      "190cf27874bc4b64af079b5a52c5faf0",
      "931fd2bdcf104e658c315a087b0cd976",
      "0ab1c0959a7c420398d5527672407b0b",
      "7185dc11f73341ebb2a7ca2b62215cc6"
     ]
    },
    "id": "sql7RSXMPd1g",
    "outputId": "3c60fce9-2844-4e76-8185-fedf8c79bc85"
   },
   "outputs": [],
   "source": [
    "model_path = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"texto\"], truncation=True, max_length=512, padding='longest', return_tensors='pt')\n",
    "\n",
    "# Mapeamento de Labels: Essencial para o modelo e m√©tricas\n",
    "id2label = {0: \"invasao_domicilio\", 1: \"violencia_fisica\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Custom Dataset (para trabalhar com tensores PyTorch diretamente)\n",
    "class CustomDataset(TorchDataset): # Certifique-se que 'Dataset' foi importado corretamente\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings # Agora ser√° um dicion√°rio de tensores (input_ids, attention_mask)\n",
    "        self.labels = labels       # Agora ser√° uma lista de labels (ou Series com √≠ndice reiniciado)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # Acesso direto pelo √≠ndice\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Data Collator para padding din√¢mico durante o treinamento\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S-nz2HQZ0CA"
   },
   "source": [
    "# **3. M√©tricas de Desempenho**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIMdebFsx3td"
   },
   "source": [
    "Para avaliar o desempenho do nosso modelo de forma completa, definimos uma s√©rie de m√©tricas. Al√©m da ***acur√°cia*** (a propor√ß√£o de acertos), calculamos o ***AUC-ROC*** (que mede a capacidade de distin√ß√£o entre as classes), e as m√©tricas de ***Precis√£o, Recall e F1-Score***. O c√°lculo dessas m√©tricas para cada classe e a m√©dia (***macro***) nos d√£o uma vis√£o detalhada do desempenho do modelo, especialmente em datasets com classes desbalanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176,
     "referenced_widgets": [
      "cd7f0f5c72244c998a71a134aa030b60",
      "dc47f52da55c4d93a53454096a1c0da9",
      "70836eb8c8004285b28190ec647610c8",
      "600c555bf3d74da2b9deefd61c7f16cb",
      "fe828c9c2ecd45d4bb4d17497b9b36d4",
      "5efad11813e5445b81d4eae74c5d7e8a",
      "6eb0716fd45b427f8eb8558971278e43",
      "b82874a2cc99440ab0ba2297a94ff39e",
      "923351fd146a4148b4befba1da9450c2",
      "0f5365c65a114380b01036659cf208ec",
      "4d1f12194710430e88405af619e0c62e",
      "afa0c268d44f4480ab4d20113fec43dd",
      "e4408609bae64a50bc8f45330ab3e850",
      "574504585abc45bf975ed37d5bb2671b",
      "b66c932f402b4a288e138b87e9b825f2",
      "a0ebfe1d3d224d568f580d91a1304f6b",
      "c960036ee9a34bf5aadaf7a188b56172",
      "3986a8d649664a21a029a821f7db8c57",
      "8e92d551ae6240f19582b759d3a06a39",
      "c6dffa900f684e72995aa7193d599018",
      "ecd59f1e03bd445a8656fc97feda085e",
      "c726f60ff5d4475fabd8e5238750c64e",
      "125065ee3d7845b9a8de77d1e44801ec",
      "45b3eb6889314b89a52e08a164f95366",
      "18b0f5d8b06c474c9f28b3bd1eab4eee",
      "705f3f110c734d6cacf7c3964e730fa7",
      "f3b6fd3b6a8f465d8c4e07de68b10c50",
      "4e6a72c7e9db49fbafe32bc3558a683a",
      "e5f5051854b54d19ba3e2806b41f4337",
      "61bf9a733c90452a92bfed9bb9af35b4",
      "8daa307f71cf43778bd959caf9bb2b92",
      "6ab7ed4eedbf47e8a1e0f0035792fd64",
      "2928d8a8e4d84d99b9bf7ad170dc492e",
      "40482c3ea0ff4a75a4cb87c58e24a60e",
      "5dc92ba91f4b4039adc63f42b9f065ab",
      "8a433ca5db8d4f59b6464fe7ecfb2c24",
      "bd0cdf217dcb48bba1ad9aeb694f0abd",
      "4798d2f2fa6647de9d1c4b9dabb905fe",
      "c77839530bbc45dcb7dba05527a09f59",
      "eb5c9080c7fe4d02a10109d8c693fd9d",
      "e4f33f783c2e49e087d6dc0b0be08f9e",
      "439ff82a2d304a4c921a671826d6efdf",
      "8f9636c9729b4fbd909ca0f00856c844",
      "1d2e9faf5c0e4d7ea91bb4b9b372c8e3",
      "d88797289e3b42e5b84f6b7bc45d650a",
      "7fdd8ca2db994b0e81e16fa37591ffd1",
      "3f4e71f98acc446ab00158b650d3a0cd",
      "ae09984dcbca44b2b7a7740ff9fb3a45",
      "fd582b0db28b408a935845e93fb6b532",
      "45dc848707a04901969de0504aa421d1",
      "de4f62134eec4d3caea268f2e42c61e1",
      "6e53ee4ce3e94d9fa5ece4e0ecf1f155",
      "b93bb59424334a73af4a9d242b60c8cb",
      "8e5fe60f5c514cd4a4253c4270745a34",
      "a38444dd27d64d0c8256571d78adbfb0"
     ]
    },
    "id": "F-gctrIbZ1Pv",
    "outputId": "e925d53f-1fd2-41db-e1bb-7bb18b6c7bf8"
   },
   "outputs": [],
   "source": [
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "auc_metric = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        probabilities = np.exp(predictions - np.max(predictions, axis=-1, keepdims=True)) / np.sum(np.exp(predictions - np.max(predictions, axis=-1, keepdims=True)), axis=-1, keepdims=True)\n",
    "        positive_class_probs = probabilities[:, 1]\n",
    "\n",
    "        # Classe prevista (ID da maior probabilidade)\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Labels das classes, conforme definido no notebook\n",
    "        class_labels = [label2id[\"invasao_domicilio\"], label2id[\"violencia_fisica\"]]\n",
    "\n",
    "        # --- C√ÅLCULO DAS M√âTRICAS ---\n",
    "        acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "        auc = auc_metric.compute(prediction_scores=positive_class_probs, references=labels)[\"roc_auc\"]\n",
    "\n",
    "        f1_results_by_class = f1_metric.compute(predictions=preds, references=labels, average=None, labels=class_labels)[\"f1\"]\n",
    "        f1_macro = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "\n",
    "        precision_results_by_class = precision_metric.compute(predictions=preds, references=labels, average=None, labels=class_labels)[\"precision\"]\n",
    "        precision_macro = precision_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"]\n",
    "\n",
    "        recall_results_by_class = recall_metric.compute(predictions=preds, references=labels, average=None, labels=class_labels)[\"recall\"]\n",
    "        recall_macro = recall_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"]\n",
    "\n",
    "        # --- RETORNO DO DICION√ÅRIO COMPLETO DE M√âTRICAS ---\n",
    "        return {\n",
    "            \"accuracy\": round(acc, 4),\n",
    "            \"auc\": round(auc, 4),\n",
    "            \"f1_invasao\": round(f1_results_by_class[0], 4),\n",
    "            \"f1_violencia\": round(f1_results_by_class[1], 4),\n",
    "            \"precision_invasao\": round(precision_results_by_class[0], 4),\n",
    "            \"precision_violencia\": round(precision_results_by_class[1], 4),\n",
    "            \"recall_invasao\": round(recall_results_by_class[0], 4),\n",
    "            \"recall_violencia\": round(recall_results_by_class[1], 4),\n",
    "            \"precision_macro\": round(precision_macro, 4),\n",
    "            \"recall_macro\": round(recall_macro, 4),\n",
    "            \"f1_macro\": round(f1_macro, 4)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no c√°lculo de m√©tricas: {str(e)}\")\n",
    "        # Em caso de erro, retorna valores padr√£o para n√£o quebrar o treinamento\n",
    "        return {\n",
    "            \"accuracy\": 0.0,\n",
    "            \"auc\": 0.0,\n",
    "            \"f1_invasao\": 0.0,\n",
    "            \"f1_violencia\": 0.0,\n",
    "            \"precision_invasao\": 0.0,\n",
    "            \"precision_violencia\": 0.0,\n",
    "            \"recall_invasao\": 0.0,\n",
    "            \"recall_violencia\": 0.0,\n",
    "            \"precision_macro\": 0.0,\n",
    "            \"recall_macro\": 0.0,\n",
    "            \"f1_macro\": 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fmwNJA-Z5DK"
   },
   "source": [
    "# **4. Valida√ß√£o Cruzada (K-Fold) com Early Stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwxxmwBqyXXV"
   },
   "source": [
    "Esta √© a etapa central do nosso projeto. Utilizamos a ***Valida√ß√£o Cruzada com 5 folds*** para treinar o modelo. Isso significa que o dataset √© dividido em 5 partes: o modelo √© treinado em 4 delas e validado na 5¬™. Este processo se repete 5 vezes, garantindo que todas as den√∫ncias sejam usadas na valida√ß√£o e nos dando uma estimativa mais precisa do desempenho geral do modelo. O ***Early Stopping*** √© uma ferramenta crucial para evitar o overfitting, interrompendo o treinamento se o modelo parar de melhorar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "06abaa6da9c54267b6972ab86cfb603b",
      "938efce3c39a4dda83be2483cbb650a0",
      "12f962d145b04674bf089de0ada1e569",
      "428555e733424518a13811be0c0e22c6",
      "ff70e5e64e844bbea499aaa4df75d8b3",
      "0fbb30d8473243edb48c3e68d5c198c6",
      "228ec2383bac48108c5ee4c2b41e8db2",
      "d208ccaa56504a68985bbb5ffd99c3e5",
      "c10b543ade29431c9a7bb04abcbaf8a9",
      "c6eb8a2d7dee49a3b8cd1dfb73ca8fe5",
      "e9099b3c3fc04287aab13186cdbeb32e",
      "3430cd37432b4ada9458e6711ee6d2b6",
      "69d31dc75caf4703b202a46a296ae99f",
      "3a7bea81f347472e9162ce9c4d51e3fa",
      "9d62c505c32c4eeb9caa9c20c42a5e3f",
      "1cfffceaf5b1439296966eafea8fc792",
      "6c021f23b3f745dd9390efba551ac695",
      "dc9aca11a4024f24a95da9ab72cfbbaf",
      "c86dbf50f39f40fba92ee848b5787482",
      "e779d6b36dfe4db09740d3550e260a9a",
      "8ddbc262442749fdb908ee7f6a00c742",
      "f70a245a24f040b3a6d40017eb06bec7"
     ]
    },
    "id": "Lu5KpVfWaGA3",
    "outputId": "7f1aff58-1a85-42e5-9fa4-9fe8112eca21"
   },
   "outputs": [],
   "source": [
    "# Essas listas armazenar√£o o hist√≥rico de m√©tricas de CADA fold para a plotagem.\n",
    "all_fold_train_losses = []\n",
    "all_fold_eval_losses = []\n",
    "all_fold_eval_accuracies = []\n",
    "# ---------------------------------------------------------------------\n",
    "\n",
    "# Par√¢metros de Treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bertimbau-denuncias-cv\", # Diret√≥rio de sa√≠da para K-Fold\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=30, # Aumentado para permitir que Early Stopping atue\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\", # Monitorar a perda de valida√ß√£o\n",
    "    greater_is_better=False, # Menor perda √© melhor\n",
    "    logging_dir=\"./logs-cv\",\n",
    "    logging_steps=10, # Diminu√≠do para ver logs mais frequentemente (era 100)\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True, # Treinamento com precis√£o mista\n",
    "    seed=42, # Semente para reprodutibilidade\n",
    "    gradient_accumulation_steps=1,\n",
    "    remove_unused_columns=True,\n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "# Preparar dados para K-Fold\n",
    "all_texts = df['texto'].tolist()\n",
    "all_labels_mapped = df['classe'].map(label2id).tolist() # Labels j√° mapeadas para IDs num√©ricos\n",
    "\n",
    "N_SPLITS = 5 # N√∫mero de folds. Para 150 registros, 5 folds √© razo√°vel.\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# Lista para armazenar as m√©tricas de teste de cada fold\n",
    "all_fold_test_metrics = []\n",
    "\n",
    "print(f\"\\nüöÄ Iniciando Treinamento com K-Fold Cross-Validation ({N_SPLITS} folds)....\\n\")\n",
    "\n",
    "# Loop atrav√©s dos folds\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(all_texts, all_labels_mapped)):\n",
    "    print(f\"\\n==================== INICIANDO FOLD {fold+1}/{N_SPLITS} ====================\")\n",
    "\n",
    "    # Dividir os dados para o fold atual\n",
    "    train_fold_texts = [all_texts[i] for i in train_index]\n",
    "    train_fold_labels = [all_labels_mapped[i] for i in train_index]\n",
    "    test_fold_texts = [all_texts[i] for i in test_index]\n",
    "    test_fold_labels = [all_labels_mapped[i] for i in test_index]\n",
    "\n",
    "    # Tokenizar os dados para o fold atual\n",
    "    train_fold_encodings = tokenize_function({\"texto\": train_fold_texts})\n",
    "    test_fold_encodings = tokenize_function({\"texto\": test_fold_texts})\n",
    "\n",
    "    # Criar CustomDatasets para o fold atual\n",
    "    train_dataset = CustomDataset(train_fold_encodings, train_fold_labels)\n",
    "    eval_dataset_for_trainer = CustomDataset(test_fold_encodings, test_fold_labels)\n",
    "\n",
    "    # Re-instanciar o modelo para cada fold para garantir pesos iniciais limpos\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=2,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    # Congelar camadas do BERT (exceto pooler e classificador)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' not in name and 'pooler' not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    print(f\"  N√∫mero de par√¢metros trein√°veis no FOLD {fold+1}: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    # Re-instanciar o Trainer para cada fold\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset_for_trainer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold = 1e-3)],\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Treinar o modelo para o fold atual\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "\n",
    "        # Salvar o best model do fold atual\n",
    "        output_dir_fold = f\"/content/drive/MyDrive/2025/tcc-final/resultados_kfold/fold-{fold+1}\"\n",
    "        trainer.save_model(output_dir_fold)\n",
    "        print(f\"‚úÖ Modelo salvo em: {output_dir_fold}\")\n",
    "\n",
    "        # Perform evaluation on the test set of the current fold\n",
    "        evaluation_output = trainer.evaluate(eval_dataset_for_trainer)\n",
    "\n",
    "        # Extract predictions and true labels\n",
    "        predictions = trainer.predict(eval_dataset_for_trainer).predictions\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        true_labels = eval_dataset_for_trainer.labels\n",
    "\n",
    "        # Map predicted labels back to original class names\n",
    "        predicted_class_names = [id2label[label] for label in predicted_labels]\n",
    "        true_class_names = [id2label[label] for label in true_labels]\n",
    "\n",
    "        # Create a DataFrame with text, true labels, and predicted labels\n",
    "        predictions_df = pd.DataFrame({\n",
    "            \"texto\": test_fold_texts,\n",
    "            \"classe_verdadeira\": true_class_names,\n",
    "            \"classe_prevista\": predicted_class_names\n",
    "        })\n",
    "\n",
    "        # Save the DataFrame to an Excel file\n",
    "        predictions_output_path = f\"/content/drive/MyDrive/2025/tcc-final/resultados_kfold/fold_{fold+1}_predictions.xlsx\"\n",
    "        predictions_df.to_excel(predictions_output_path, index=False)\n",
    "        print(f\"‚úÖ Previs√µes salvas em: {predictions_output_path}\")\n",
    "\n",
    "\n",
    "        # --- NOVA L√ìGICA DE COLETA DE M√âTRICAS PARA PLOTAGEM ---\n",
    "        # Extrair o hist√≥rico de logs do Trainer para o fold atual\n",
    "        logs = trainer.state.log_history\n",
    "\n",
    "        # Listas tempor√°rias para as m√©tricas deste fold\n",
    "        fold_train_losses, fold_eval_losses, fold_eval_accuracies = [], [], []\n",
    "\n",
    "        for log_entry in logs:\n",
    "            if 'loss' in log_entry and 'eval_loss' not in log_entry:\n",
    "                fold_train_losses.append(log_entry['loss'])\n",
    "            if 'eval_loss' in log_entry:\n",
    "                fold_eval_losses.append(log_entry['eval_loss'])\n",
    "                fold_eval_accuracies.append(log_entry['eval_accuracy'])\n",
    "\n",
    "        # Armazenar as curvas de m√©tricas deste fold nas listas globais\n",
    "        all_fold_train_losses.append(fold_train_losses)\n",
    "        all_fold_eval_losses.append(fold_eval_losses)\n",
    "        all_fold_eval_accuracies.append(fold_eval_accuracies)\n",
    "        # --------------------------------------------------------\n",
    "\n",
    "        print(f\"üìä M√©tricas finais de treino para FOLD {fold+1}:\")\n",
    "        print(f\"Loss: {train_result.metrics.get('train_loss', 'N/A'):.4f}\")\n",
    "        print(f\"Tempo total: {train_result.metrics.get('train_runtime', 'N/A'):.2f}s\")\n",
    "\n",
    "        # Evaluate on the test/validation set of the fold\n",
    "        print(f\"üß™ Avalia√ß√£o no conjunto de teste/valida√ß√£o do FOLD {fold+1}...\")\n",
    "        fold_test_metrics = trainer.evaluate(eval_dataset_for_trainer)\n",
    "        all_fold_test_metrics.append(fold_test_metrics)\n",
    "        print(f\"Resultados do FOLD {fold+1}: {fold_test_metrics}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"‚ùå Erro durante o treinamento do FOLD {fold+1}:\")\n",
    "        print(traceback.format_exc())\n",
    "        all_fold_test_metrics.append({\n",
    "            'eval_loss': float('nan'),\n",
    "            'eval_accuracy': 0.0,\n",
    "            'eval_auc': 0.0,\n",
    "            'eval_f1_invasao': 0.0,\n",
    "            'eval_f1_violencia': 0.0,\n",
    "            'eval_runtime': float('nan'),\n",
    "            'eval_samples_per_second': 0.0,\n",
    "            'eval_steps_per_second': 0.0\n",
    "        })\n",
    "\n",
    "# --- C√ìDIGO DE PLOTAGEM AJUSTADO PARA K-FOLD ---\n",
    "\n",
    "max_epochs = max(len(h) for h in all_fold_eval_losses)\n",
    "epochs = list(range(1, max_epochs + 1))\n",
    "\n",
    "# Criar a figura com dois subplots (1 linha, 2 colunas)\n",
    "# Aumentamos o tamanho para melhor visualiza√ß√£o.\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "for i, train_loss_history in enumerate(all_fold_train_losses):\n",
    "    ax1.plot(range(1, len(all_fold_eval_losses[i]) + 1),\n",
    "             all_fold_train_losses[i][:len(all_fold_eval_losses[i])],\n",
    "             marker='x', linestyle='--', label=f'Fold {i+1}')\n",
    "\n",
    "ax1.set_xlabel(\"√âpoca\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Perda de Treino por √âpoca (Todos os Folds)\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# --- Gr√°fico 2: Perda de Valida√ß√£o por √âpoca ---\n",
    "# Este gr√°fico j√° estava correto, agora com a escala do eixo X ajustada\n",
    "for i, eval_loss_history in enumerate(all_fold_eval_losses):\n",
    "    ax2.plot(range(1, len(eval_loss_history) + 1), eval_loss_history, marker='o', linestyle='-', label=f'Fold {i+1}')\n",
    "\n",
    "ax2.set_xlabel(\"√âpoca\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_title(\"Perda de Valida√ß√£o por √âpoca (Todos os Folds)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Ajusta o layout para evitar sobreposi√ß√£o de t√≠tulos e r√≥tulos\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFSsNAWvaQQs"
   },
   "source": [
    "# **5. RESULTADOS FINAIS DA VALIDA√á√ÉO CRUZADA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a5gLzc6aQuU",
    "outputId": "d8205c02-a3b3-4145-fb84-f6f2f60f18b8"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n==================== RESULTADOS FINAIS K-FOLD CROSS-VALIDATION ====================\")\n",
    "\n",
    "if all_fold_test_metrics:\n",
    "    # Coletar todas as m√©tricas de todos os folds\n",
    "    df_metrics = pd.DataFrame(all_fold_test_metrics)\n",
    "\n",
    "    # Calcular m√©dias e desvios padr√£o\n",
    "    avg_metrics = df_metrics.mean(numeric_only=True).round(4).to_dict()\n",
    "    std_metrics = df_metrics.std(numeric_only=True).round(4).to_dict()\n",
    "\n",
    "    print(\"M√©tricas M√©dias (e Desvio Padr√£o) em todos os Folds:\")\n",
    "    for metric, avg_value in avg_metrics.items():\n",
    "        # Excluir m√©tricas de tempo/desempenho por segundo da apresenta√ß√£o principal se desejar\n",
    "        if 'runtime' not in metric and 'samples_per_second' not in metric and 'steps_per_second' not in metric:\n",
    "            print(f\"{metric}: {avg_value} (¬± {std_metrics.get(metric, 0)})\")\n",
    "else:\n",
    "    print(\"Nenhum resultado de fold foi coletado. Ocorreram erros em todos os folds.\")\n",
    "\n",
    "print(\"\\n‚úÖ K-Fold Cross-Validation Conclu√≠do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SLL_XiZaSle"
   },
   "source": [
    "# **6. SALVAR MODELO (Considera√ß√µes P√≥s K-Fold)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbEnj8O5aUmx",
    "outputId": "3a6cc7fa-ad73-4c53-84a7-12ab0b275dae"
   },
   "outputs": [],
   "source": [
    "criterio = \"eval_accuracy\"  # ou \"eval_auc\", \"eval_f1_invasao\", etc.\n",
    "best_fold_idx = int(np.argmax([m.get(criterio, float('-inf')) for m in all_fold_test_metrics]))\n",
    "print(f\"üèÜ Melhor modelo: Fold {best_fold_idx+1} com {criterio} = {all_fold_test_metrics[best_fold_idx][criterio]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
