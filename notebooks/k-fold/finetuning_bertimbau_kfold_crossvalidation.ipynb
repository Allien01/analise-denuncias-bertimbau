{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsCCzLwYbKDF"
   },
   "source": [
    "@inproceedings{souza2020bertimbau, author = {F{'a}bio Souza and Rodrigo Nogueira and Roberto Lotufo}, title = {{BERT}imbau: pretrained {BERT} models for {B}razilian {P}ortuguese}, booktitle = {9th Brazilian Conference on Intelligent Systems, {BRACIS}, Rio Grande do Sul, Brazil, October 20-23 (to appear)}, year = {2020} }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VU_ldpdMEMbw",
    "outputId": "f179a409-2af5-4a8a-bd1a-938da0fb345a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0BkkHsJY5z-5",
    "outputId": "c83ef15d-9cc1-4926-ad3f-5cb429974c0c"
   },
   "outputs": [],
   "source": [
    "!pip install nbstripout\n",
    "!nbstripout /content/drive/MyDrive/2025/tcc-final/fine_tuning_Bertimbau_version2_24_07.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "feHRGfqdONsa",
    "outputId": "9bec9b78-4e10-4072-e3da-5275c60f8b81"
   },
   "outputs": [],
   "source": [
    "!pip install transformers evaluate accelerate\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback # Importa explicitamente para o callback\n",
    ")\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import subprocess\n",
    "import time\n",
    "import evaluate\n",
    "start_total_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkRUcLU1PHke"
   },
   "source": [
    "# **1. Prepara√ß√£o do Dataset para K-Fold**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gG-9Pr_Mw3jU"
   },
   "source": [
    "Iniciamos carregando o dataset completo de den√∫ncias. Diferente do m√©todo anterior, aqui n√£o faremos uma divis√£o fixa. Em vez disso, usaremos a ***t√©cnica de Valida√ß√£o Cruzada (K-Fold)***, que nos permite treinar e validar o modelo v√°rias vezes em subconjuntos diferentes do nosso dataset, obtendo uma avalia√ß√£o de desempenho mais confi√°vel e robusta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3O3x60E3PHtT",
    "outputId": "da9240e6-0bb6-41ff-a5e8-d9a289356ec6"
   },
   "outputs": [],
   "source": [
    "# Carregar dataset j√° processado\n",
    "file_path = '/content/drive/MyDrive/2025/tcc-final/denuncias_balanceadas.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "print(f\"Dataset carregado. Total de registros: {len(df)}\")\n",
    "print(\"Primeiras 5 linhas do dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\nInforma√ß√µes do dataset:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIeftoByY356"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DE3WAEgOPYHa"
   },
   "source": [
    "# **2. Processando o Texto com o BERTimbau**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AoC0YTBEYuA6"
   },
   "source": [
    "Assim como na abordagem anterior, esta etapa √© crucial para preparar o texto. Usamos o ***tokenizer*** do modelo ***BERTimbau*** para converter as den√∫ncias em um formato num√©rico compreens√≠vel pelo modelo. Tamb√©m criamos classes personalizadas ***(CustomDataset)*** e um *** DataCollator*** para otimizar a forma como os dados s√£o alimentados ao modelo durante o treinamento e a avalia√ß√£o.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275,
     "referenced_widgets": [
      "2c42eea389e94104862c334561460765",
      "d6d5466791dc4b1a821669b304078bc6",
      "8783805f85694de2b10f88957428d67d",
      "9326b407cb1f4ecf9c4e84bffbe1b528",
      "dc132a9250784816b7230542cdb45889",
      "d551e302d3b448709562ce81a6ff760b",
      "52fbf332c315455580730b591e13c73d",
      "33b34fa1c7774aaba6ad77469397945a",
      "09e8345d37a74c36a909d9f96c55b869",
      "95c6daa1079042aa83169b56aef8dfb4",
      "9ee7105729b74fcbae815e8274247feb",
      "5ef0607f4eec44cbaf9ffdd8e4d9f9f6",
      "a234b707a7544f67890777ea22e94a01",
      "2148a415d08b4432b16b6e4e3f3ebf74",
      "efb10a28edd44059a6f0297b8085ee59",
      "d55a6c4f030947f283e6ae988febf555",
      "38efd2bde201408fa810c576f452d5f3",
      "879122aa39eb422c9677d8f888898725",
      "37bd97f087234b5d96990fa747dc2789",
      "16f9b0e98bd94783a88b8715212bdf21",
      "a300e26ec47e4090965e7b8d28966c7a",
      "5265ae08f5974c9faf686f3b5843d482",
      "704b06773246498d92237cdaa9a7e9d7",
      "7538e30f770346398d5cad3a68961423",
      "c6936e59ca75455096db585388de6743",
      "0714f5a7ab03471988605d633af9fee5",
      "284ea584beff44aeadbcb80c306d59db",
      "3ac3c83a2e0c410aa7b3fa62492e8d49",
      "f15b96ce70724dc9bc808a3702d74189",
      "ff25b95d280f43179b86d65e8cf49f46",
      "5adf24895fbe44ba8ab71a5e585cf93c",
      "e80f1d27f70849058fa9e2a669688d32",
      "12040ec922e8406382d5658885dd1097",
      "88e85366f970489398c7f946ad591c58",
      "ffb9ca211d4f4804bd6ae697a14099cf",
      "f72991919bde4431ac3421b8f632174c",
      "a568b1dd32434efcb1cca08230a1515b",
      "87fe2021f28b40ed88afca1eac76aaac",
      "3bcda167e70c42ffbc2f8ad7c00962e7",
      "78ae2b6982da465f96df5ef9abf9a5fe",
      "36d394526bc84b46985d17efffb350e1",
      "818a5e38a897478c8694d7a5aca761da",
      "6d3c8fb2ffd440ba83fd2b12ce858dab",
      "0d96eabf4cf3430ab792f6404abd4e2d",
      "d0837acdba764497875de79e88f5fcee",
      "a1b7cb83f2ab4d5aa151087148e3cdd5",
      "eb0645875e9745ac88368c7aaf211044",
      "91945844cf344646b38d334e2f636ddd",
      "cc7cb5031afe4a1aa6fbec6446484f39",
      "860eaed505874271ad32757c4e0d2379",
      "0d0a7edec6f443c68ac025c2610c6534",
      "e64af848a862433cadc2f9573842ac1c",
      "5d1a108d07704298ac5b5c81eaa49ac9",
      "0bd791cca87f488d99525dbd8113e439",
      "d50495f1b9d145f286ce0ed80abdc450"
     ]
    },
    "id": "sql7RSXMPd1g",
    "outputId": "dbb94a81-3c68-4343-882b-cd619d99b673"
   },
   "outputs": [],
   "source": [
    "model_path = \"neuralmind/bert-base-portuguese-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "def tokenize_function(batch):\n",
    "    return tokenizer(batch[\"texto\"], truncation=True, max_length=512, padding='longest', return_tensors='pt')\n",
    "\n",
    "# Mapeamento de Labels: Essencial para o modelo e m√©tricas\n",
    "id2label = {0: \"invasao_domicilio\", 1: \"violencia_fisica\"}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "# Custom Dataset (para trabalhar com tensores PyTorch diretamente)\n",
    "class CustomDataset(TorchDataset): # Certifique-se que 'Dataset' foi importado corretamente\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings # Agora ser√° um dicion√°rio de tensores (input_ids, attention_mask)\n",
    "        self.labels = labels       # Agora ser√° uma lista de labels (ou Series com √≠ndice reiniciado)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long) # Acesso direto pelo √≠ndice\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Data Collator para padding din√¢mico durante o treinamento\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2S-nz2HQZ0CA"
   },
   "source": [
    "# **3. M√©tricas de Desempenho**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIMdebFsx3td"
   },
   "source": [
    "Para avaliar o desempenho do nosso modelo de forma completa, definimos uma s√©rie de m√©tricas. Al√©m da ***acur√°cia*** (a propor√ß√£o de acertos), calculamos o ***AUC-ROC*** (que mede a capacidade de distin√ß√£o entre as classes), e as m√©tricas de ***Precis√£o, Recall e F1-Score***. O c√°lculo dessas m√©tricas para cada classe e a m√©dia (***macro***) nos d√£o uma vis√£o detalhada do desempenho do modelo, especialmente em datasets com classes desbalanceadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176,
     "referenced_widgets": [
      "dfa89535bbd44b9890cf733f34d7695e",
      "a7d7eec01b544a79b043391d378a1e67",
      "92db85630c494dbcb84f55850a8d3af3",
      "ef32e30858e2407092540e4574042a22",
      "82840f8e7df24fd1a5e2649d85c12361",
      "ac3d021d1a9f4a249b807a78179ecc27",
      "e36dedaf2da04643a1a9600a90b953bb",
      "ee6bea35a7f541f6a7e41c16afc823ea",
      "e954d7cfedec48cd8d28e257783d4313",
      "90d1ab4f6ac84c67a56c8bdaf8d1f03a",
      "3ccc3196464d4e03aa9c325e0bf23c59",
      "a7c59294874743aa879941be64b6e413",
      "6240d77a00ae48f5b3647c5cdd8a3d11",
      "13be3751bcaa4c5ab41c8d1311eec8af",
      "07b900906b5242db9321b59faffd3e16",
      "843a27342d4343caa115e5b5acf63b68",
      "5ca0fe4a3b084a06922123f8f3c0d5ec",
      "7af7308b263d4128905348ab02dd2d6b",
      "635294751118487c88e68c9196ebfc76",
      "33d5964deb9f4f9fa19014c185a7fe10",
      "1b1d58f9759e406f989fec5c65a8c224",
      "7c0fa94e2338473290d45c8b94bccfd4",
      "9f9f4f928bdb47bfbc3a56a089ade7cc",
      "da3f5841d5fb4d178c5c728a2d9c54e7",
      "73fd7b35fe834316a6f99af3109e5a1a",
      "cc60ddae66e64770a9e5bb170552d6d9",
      "663604f43b5144318b7dae13c12d38dd",
      "66d248ab3b694092b8633a86a5128457",
      "53d0bd1dd8974f10a6621fda4ae7709e",
      "cb766c769f034a999ac8db22a0ac1203",
      "805d751575ab4b9cb1410d1bdd589b34",
      "23a67032316141b9a389f7b7230a564f",
      "d5c16bd5b56e4fc09b1417e6ca48317d",
      "4501e156008e461bbf9334cbe46d1062",
      "2ae8eda72c4c423fad7f77b5482d341d",
      "68d811f823a946a794afeca84b3da995",
      "e1af42096b7e44c3979e43152092fc4d",
      "e4d5f0bc8b4042838f9d08a41a5535cc",
      "0bb2fff6a0364c77951c7f2748f4a163",
      "dedc2a05733f4ddb8fa657e29514a43a",
      "75d43798477c4134a53b126a0831f897",
      "bbbdd751577c4a7bb0ccfe53ae74cde9",
      "54a771f032034ee091786036ceb3afb5",
      "d871c89ccc3c4a69baafe586a1563613",
      "b2dfb4398b224ad1aa632bb24c50f5fe",
      "663dbd7e8e744cafba7769abe4e99fd8",
      "c2de89f789ac4793b9e7901dd0a64c31",
      "34e0cc6d260e4df499c9a8b514fa943e",
      "5bada55a6fd94d019341997d20898b42",
      "f39cf1a534fb4fdc9181c637ba76c3e1",
      "8aeb8ce0126744f99137ceea1aaf5538",
      "bf76d9c64bc64778af3ac9cc2a093160",
      "fd62f68c1dbf42a89d90130a937e8d58",
      "8fb20bab60424d5f811d7335a2e3a5ab",
      "a44bcd2ad68643bc95f8ac4a7a236023"
     ]
    },
    "id": "F-gctrIbZ1Pv",
    "outputId": "c14d8d9c-6807-48af-f8e1-9c29d4a0a5d8"
   },
   "outputs": [],
   "source": [
    "precision_metric = evaluate.load(\"precision\")\n",
    "recall_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "auc_metric = evaluate.load(\"roc_auc\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    try:\n",
    "        predictions, labels = eval_pred\n",
    "        probabilities = np.exp(predictions - np.max(predictions, axis=-1, keepdims=True)) / np.sum(np.exp(predictions - np.max(predictions, axis=-1, keepdims=True)), axis=-1, keepdims=True)\n",
    "        positive_class_probs = probabilities[:, 1]\n",
    "\n",
    "        # Classe prevista (ID da maior probabilidade)\n",
    "        preds = np.argmax(predictions, axis=1)\n",
    "\n",
    "        # Labels das classes, conforme definido no notebook\n",
    "        class_labels = [label2id[\"invasao_domicilio\"], label2id[\"violencia_fisica\"]]\n",
    "\n",
    "        # --- C√ÅLCULO DAS M√âTRICAS ---\n",
    "        acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "        auc = auc_metric.compute(prediction_scores=positive_class_probs, references=labels)[\"roc_auc\"]\n",
    "\n",
    "        f1_results_by_class = f1_metric.compute(predictions=preds, references=labels, average=None, labels=class_labels)[\"f1\"]\n",
    "        f1_macro = f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "\n",
    "        precision_results_by_class = precision_metric.compute(predictions=preds, references=labels, average=None, labels=class_labels)[\"precision\"]\n",
    "        precision_macro = precision_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"]\n",
    "\n",
    "        recall_results_by_class = recall_metric.compute(predictions=preds, references=labels, average=None, labels=class_labels)[\"recall\"]\n",
    "        recall_macro = recall_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"]\n",
    "\n",
    "        # --- RETORNO DO DICION√ÅRIO COMPLETO DE M√âTRICAS ---\n",
    "        return {\n",
    "            \"accuracy\": round(acc, 4),\n",
    "            \"auc\": round(auc, 4),\n",
    "            \"f1_invasao\": round(f1_results_by_class[0], 4),\n",
    "            \"f1_violencia\": round(f1_results_by_class[1], 4),\n",
    "            \"precision_invasao\": round(precision_results_by_class[0], 4),\n",
    "            \"precision_violencia\": round(precision_results_by_class[1], 4),\n",
    "            \"recall_invasao\": round(recall_results_by_class[0], 4),\n",
    "            \"recall_violencia\": round(recall_results_by_class[1], 4),\n",
    "            \"precision_macro\": round(precision_macro, 4),\n",
    "            \"recall_macro\": round(recall_macro, 4),\n",
    "            \"f1_macro\": round(f1_macro, 4)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no c√°lculo de m√©tricas: {str(e)}\")\n",
    "        # Em caso de erro, retorna valores padr√£o para n√£o quebrar o treinamento\n",
    "        return {\n",
    "            \"accuracy\": 0.0,\n",
    "            \"auc\": 0.0,\n",
    "            \"f1_invasao\": 0.0,\n",
    "            \"f1_violencia\": 0.0,\n",
    "            \"precision_invasao\": 0.0,\n",
    "            \"precision_violencia\": 0.0,\n",
    "            \"recall_invasao\": 0.0,\n",
    "            \"recall_violencia\": 0.0,\n",
    "            \"precision_macro\": 0.0,\n",
    "            \"recall_macro\": 0.0,\n",
    "            \"f1_macro\": 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fmwNJA-Z5DK"
   },
   "source": [
    "# **4. Valida√ß√£o Cruzada (K-Fold) com Early Stopping**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwxxmwBqyXXV"
   },
   "source": [
    "Esta √© a etapa central do nosso projeto. Utilizamos a ***Valida√ß√£o Cruzada com 5 folds*** para treinar o modelo. Isso significa que o dataset √© dividido em 5 partes: o modelo √© treinado em 4 delas e validado na 5¬™. Este processo se repete 5 vezes, garantindo que todas as den√∫ncias sejam usadas na valida√ß√£o e nos dando uma estimativa mais precisa do desempenho geral do modelo. O ***Early Stopping*** √© uma ferramenta crucial para evitar o overfitting, interrompendo o treinamento se o modelo parar de melhorar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "3063756ade9a4f0c8e468337a14aba9d",
      "1aa4ae944fbc49dfad4734d4fc58b6c9",
      "2830e2b69d2948f9976471528050ab78",
      "247f4b5e4cc743ae9999c3984a05bc93",
      "d360f2fa8a1943d68da055bd037ac24d",
      "a869ed1b5000422aace2792dd92b9a73",
      "d42d71fdca274ebf9bfceec56c7e00c5",
      "4e2469bdeb9141659ee9cc7271da00e6",
      "3303da04222e490fb5159d77d74e575f",
      "efbf9228b1f9427a9b5933873e839c47",
      "dc16267784444d5591a5c65f66155b55",
      "c67090952d5c4b198d6f4d9fe7521b93",
      "6887bbe983894244b2ac05553601946a",
      "cf5c10578304464bbe7ced6f9c3d12b1",
      "e6f5d1189e2d4e84acd361ffccc46776",
      "1b955f534c4f420baf1af882fe84b416",
      "f8d9ea8a27e446edb601e225004adac3",
      "910f271b45314dbd9de4bf2ae6e93144",
      "2463d1fd4c0346f28f23fcfef01841ff",
      "d9725504f0634a18a4a47d2cd20eab0c",
      "19acc0577a2748a2b754ee2320afb0f9",
      "27349e7d62e04b28adcdfbc12562b5da"
     ]
    },
    "id": "Vf6OgrwXH9QE",
    "outputId": "d29230e6-8607-447b-956b-217c7ea9c3af"
   },
   "outputs": [],
   "source": [
    "# Essas listas armazenar√£o o hist√≥rico de m√©tricas de CADA fold para a plotagem.\n",
    "all_fold_train_losses = []\n",
    "all_fold_eval_losses = []\n",
    "all_fold_eval_accuracies = []\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Par√¢metros de Treinamento\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bertimbau-denuncias-cv\", # Diret√≥rio de sa√≠da para K-Fold\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=30, # Aumentado para permitir que Early Stopping atue\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\", # Monitorar a perda de valida√ß√£o\n",
    "    greater_is_better=False, # Menor perda √© melhor\n",
    "    logging_dir=\"./logs-cv\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    disable_tqdm=False,\n",
    "    report_to=\"none\",\n",
    "    fp16=True, # Treinamento com precis√£o mista\n",
    "    seed=42, # Semente para reprodutibilidade\n",
    "    gradient_accumulation_steps=1,\n",
    "    remove_unused_columns=True,\n",
    "    label_names=[\"labels\"]\n",
    ")\n",
    "\n",
    "# Preparar dados para K-Fold\n",
    "all_texts = df['texto'].tolist()\n",
    "all_labels_mapped = df['classe'].map(label2id).tolist() # Labels j√° mapeadas para IDs num√©ricos\n",
    "\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "\n",
    "# Lista para armazenar as m√©tricas de teste de cada fold\n",
    "all_fold_test_metrics = []\n",
    "print(f\"\\nüöÄ Iniciando Treinamento com K-Fold Cross-Validation ({N_SPLITS} folds)....\\n\")\n",
    "\n",
    "# Loop atrav√©s dos folds\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(all_texts, all_labels_mapped)):\n",
    "    print(f\"\\n==================== INICIANDO FOLD {fold+1}/{N_SPLITS} ====================\")\n",
    "\n",
    "    # Dividir os dados para o fold atual\n",
    "    train_fold_texts = [all_texts[i] for i in train_index]\n",
    "    train_fold_labels = [all_labels_mapped[i] for i in train_index]\n",
    "    test_fold_texts = [all_texts[i] for i in test_index]\n",
    "    test_fold_labels = [all_labels_mapped[i] for i in test_index]\n",
    "\n",
    "    # Tokenizar os dados para o fold atual\n",
    "    train_fold_encodings = tokenize_function({\"texto\": train_fold_texts})\n",
    "    test_fold_encodings = tokenize_function({\"texto\": test_fold_texts})\n",
    "\n",
    "    # Criar CustomDatasets para o fold atual\n",
    "    train_dataset = CustomDataset(train_fold_encodings, train_fold_labels)\n",
    "    eval_dataset_for_trainer = CustomDataset(test_fold_encodings, test_fold_labels)\n",
    "\n",
    "    # Re-instanciar o modelo para cada fold\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path,\n",
    "        num_labels=2,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "\n",
    "    # Congelar camadas do BERT (exceto pooler e classificador)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'classifier' not in name and 'pooler' not in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    print(f\"‚öôÔ∏è N√∫mero de par√¢metros trein√°veis no FOLD {fold+1}: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
    "\n",
    "    # Re-instanciar o Trainer para cada fold\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset_for_trainer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3, early_stopping_threshold=1e-3)],\n",
    "        compute_metrics=compute_metrics,\n",
    "        data_collator=data_collator\n",
    "    )\n",
    "\n",
    "    # Treinar o modelo para o fold atual\n",
    "    try:\n",
    "        train_result = trainer.train()\n",
    "\n",
    "        # Salvar o best model do fold atual\n",
    "        output_dir_fold = f\"/content/drive/MyDrive/2025/tcc-final/resultados_kfold/fold-{fold+1}\"\n",
    "        trainer.save_model(output_dir_fold)\n",
    "        print(f\"‚úÖ Modelo salvo em: {output_dir_fold}\")\n",
    "\n",
    "        # Perform evaluation on the test set\n",
    "        evaluation_output = trainer.evaluate(eval_dataset_for_trainer)\n",
    "\n",
    "        # Extract predictions and true labels\n",
    "        predictions = trainer.predict(eval_dataset_for_trainer).predictions\n",
    "        predicted_labels = np.argmax(predictions, axis=1)\n",
    "        true_labels = eval_dataset_for_trainer.labels\n",
    "\n",
    "        # Map predicted labels back to original class names\n",
    "        predicted_class_names = [id2label[label] for label in predicted_labels]\n",
    "        true_class_names = [id2label[label] for label in true_labels]\n",
    "\n",
    "        # Create a DataFrame with predictions\n",
    "        predictions_df = pd.DataFrame({\n",
    "            \"texto\": test_fold_texts,\n",
    "            \"classe_verdadeira\": true_class_names,\n",
    "            \"classe_prevista\": predicted_class_names\n",
    "        })\n",
    "\n",
    "        # Save predictions to Excel\n",
    "        predictions_output_path = f\"/content/drive/MyDrive/2025/tcc-final/resultados_kfold/fold_{fold+1}_predictions.xlsx\"\n",
    "        predictions_df.to_excel(predictions_output_path, index=False)\n",
    "        print(f\"‚úÖ Previs√µes salvas em: {predictions_output_path}\")\n",
    "\n",
    "        # --- L√ìGICA SIMPLIFICADA DE COLETA DE M√âTRICAS ---\n",
    "        logs = trainer.state.log_history\n",
    "\n",
    "        # Listas tempor√°rias para as m√©tricas deste fold\n",
    "        fold_train_losses = []\n",
    "        fold_eval_losses = []\n",
    "        fold_eval_accuracies = []\n",
    "\n",
    "        # Agora √© simples: 1 log por √©poca para cada m√©trica\n",
    "        for log_entry in logs:\n",
    "            if 'loss' in log_entry and 'eval_loss' not in log_entry:\n",
    "                fold_train_losses.append(log_entry['loss'])\n",
    "            if 'eval_loss' in log_entry:\n",
    "                fold_eval_losses.append(log_entry['eval_loss'])\n",
    "                fold_eval_accuracies.append(log_entry['eval_accuracy'])\n",
    "\n",
    "        # üîç DIAGN√ìSTICO: Verificar se est√° tudo alinhado\n",
    "        print(f\"\\nüìä FOLD {fold+1} - M√©tricas coletadas:\")\n",
    "        print(f\"   ‚Ä¢ Train losses: {len(fold_train_losses)} √©pocas\")\n",
    "        print(f\"   ‚Ä¢ Eval losses: {len(fold_eval_losses)} √©pocas\")\n",
    "        print(f\"   ‚Ä¢ Eval accuracies: {len(fold_eval_accuracies)} √©pocas\")\n",
    "\n",
    "        # ‚ö†Ô∏è SEGURAN√áA: Garantir mesmo tamanho (caso haja alguma inconsist√™ncia)\n",
    "        min_length = min(len(fold_train_losses), len(fold_eval_losses))\n",
    "        if len(fold_train_losses) != len(fold_eval_losses):\n",
    "            print(f\"   ‚ö†Ô∏è AVISO: Tamanhos diferentes! Ajustando para {min_length} √©pocas\")\n",
    "            fold_train_losses = fold_train_losses[:min_length]\n",
    "            fold_eval_losses = fold_eval_losses[:min_length]\n",
    "            fold_eval_accuracies = fold_eval_accuracies[:min_length]\n",
    "\n",
    "        # Armazenar as curvas de m√©tricas deste fold\n",
    "        all_fold_train_losses.append(fold_train_losses)\n",
    "        all_fold_eval_losses.append(fold_eval_losses)\n",
    "        all_fold_eval_accuracies.append(fold_eval_accuracies)\n",
    "        # --------------------------------------------------------\n",
    "\n",
    "        print(f\"\\nüìä M√©tricas finais de treino para FOLD {fold+1}:\")\n",
    "        print(f\"   Loss: {train_result.metrics.get('train_loss', 'N/A'):.4f}\")\n",
    "        print(f\"   Tempo total: {train_result.metrics.get('train_runtime', 'N/A'):.2f}s\")\n",
    "\n",
    "        # Evaluate on test set\n",
    "        print(f\"üß™ Avalia√ß√£o no conjunto de teste/valida√ß√£o do FOLD {fold+1}...\")\n",
    "        fold_test_metrics = trainer.evaluate(eval_dataset_for_trainer)\n",
    "        all_fold_test_metrics.append(fold_test_metrics)\n",
    "        print(f\"   Resultados do FOLD {fold+1}: {fold_test_metrics}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"‚ùå Erro durante o treinamento do FOLD {fold+1}:\")\n",
    "        print(traceback.format_exc())\n",
    "        all_fold_test_metrics.append({\n",
    "            'eval_loss': float('nan'),\n",
    "            'eval_accuracy': 0.0,\n",
    "            'eval_auc': 0.0,\n",
    "            'eval_f1_invasao': 0.0,\n",
    "            'eval_f1_violencia': 0.0,\n",
    "            'eval_runtime': float('nan'),\n",
    "            'eval_samples_per_second': 0.0,\n",
    "            'eval_steps_per_second': 0.0\n",
    "        })\n",
    "\n",
    "# --- C√ìDIGO DE PLOTAGEM SIMPLIFICADO ---\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "# --- Gr√°fico 1: Perda de Treino por √âpoca ---\n",
    "for i, train_loss_history in enumerate(all_fold_train_losses):\n",
    "    epochs = range(1, len(train_loss_history) + 1)\n",
    "    ax1.plot(epochs, train_loss_history,\n",
    "             marker='x', linestyle='--', label=f'Fold {i+1}')\n",
    "\n",
    "ax1.set_xlabel(\"√âpoca\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "ax1.set_title(\"Perda de Treino por √âpoca (Todos os Folds)\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# --- Gr√°fico 2: Perda de Valida√ß√£o por √âpoca ---\n",
    "for i, eval_loss_history in enumerate(all_fold_eval_losses):\n",
    "    epochs = range(1, len(eval_loss_history) + 1)\n",
    "    ax2.plot(epochs, eval_loss_history,\n",
    "             marker='o', linestyle='-', label=f'Fold {i+1}')\n",
    "\n",
    "ax2.set_xlabel(\"√âpoca\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "ax2.set_title(\"Perda de Valida√ß√£o por √âpoca (Todos os Folds)\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFSsNAWvaQQs"
   },
   "source": [
    "# **5. RESULTADOS FINAIS DA VALIDA√á√ÉO CRUZADA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0a5gLzc6aQuU",
    "outputId": "4d89d649-570a-47dc-c9e3-9598ec0bb76a"
   },
   "outputs": [],
   "source": [
    "print(\"\\n\\n==================== RESULTADOS FINAIS K-FOLD CROSS-VALIDATION ====================\")\n",
    "\n",
    "if all_fold_test_metrics:\n",
    "    # Coletar todas as m√©tricas de todos os folds\n",
    "    df_metrics = pd.DataFrame(all_fold_test_metrics)\n",
    "\n",
    "    # Calcular m√©dias e desvios padr√£o\n",
    "    avg_metrics = df_metrics.mean(numeric_only=True).round(4).to_dict()\n",
    "    std_metrics = df_metrics.std(numeric_only=True).round(4).to_dict()\n",
    "\n",
    "    print(\"M√©tricas M√©dias (e Desvio Padr√£o) em todos os Folds:\")\n",
    "    for metric, avg_value in avg_metrics.items():\n",
    "        # Excluir m√©tricas de tempo/desempenho por segundo da apresenta√ß√£o principal se desejar\n",
    "        if 'runtime' not in metric and 'samples_per_second' not in metric and 'steps_per_second' not in metric:\n",
    "            print(f\"{metric}: {avg_value} (¬± {std_metrics.get(metric, 0)})\")\n",
    "else:\n",
    "    print(\"Nenhum resultado de fold foi coletado. Ocorreram erros em todos os folds.\")\n",
    "\n",
    "print(\"\\n‚úÖ K-Fold Cross-Validation Conclu√≠do.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1SLL_XiZaSle"
   },
   "source": [
    "# **6. SALVAR MODELO (Considera√ß√µes P√≥s K-Fold)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PbEnj8O5aUmx",
    "outputId": "d0f53e60-08b5-4deb-ecd8-ae2dd8aaa671"
   },
   "outputs": [],
   "source": [
    "criterio = \"eval_accuracy\"  # ou \"eval_auc\", \"eval_f1_invasao\", etc.\n",
    "best_fold_idx = int(np.argmax([m.get(criterio, float('-inf')) for m in all_fold_test_metrics]))\n",
    "print(f\"üèÜ Melhor modelo: Fold {best_fold_idx+1} com {criterio} = {all_fold_test_metrics[best_fold_idx][criterio]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
